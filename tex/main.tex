\documentclass{article}
\input{template.tex}

\title{Developing competences on theorical and applied aspects}
\author{Quoc-Bao Do \\
5th Year Student, Department of Applied Mathematics, INSA Toulouse, France \\
Mentor: Pierre Weiss \\
Researcher, Integrative Biology Center, Toulouse, France}
\date{January 2025}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section*{Introduction}
In this report, I present my two recent projects that I had have contributed/worked on during my internship and my 5th year at INSA Toulouse. These two projects are under the supervision of my mentor and in collaboration with researchers from the Integrative Biology Center (CBI) in Toulouse, France. The first project consists of analytically analyzing the convolutional neural networks (CNNs) under the len of Minimum Mean Square Error (MMSE) estimator. We aim to gain insights into their performance, generalization capabilities, and their sensibility to various factors. This theoretical foundation will help inform the design and optimization of CNN architectures for various applications. The second project aims to develop a web-based application that utilizes deep learning techniques to detect and classify cyanobacterias from microscope images. While these two projects may seem distinct, they are done under the neccesity of advanced technologies that can solve real-world problems that the biologists and researchers from CBI are facing. In other hands, these two works form my set of skills both in theoretical and applied aspects, which are equivalently essential for my future career as a researcher in applied mathematics.

This report is structured as follows: Section~\ref{MMSE_analytical} presents the summary of the first project, including the mathematical formation, result, experimental validation and conclusion. Section~\ref{cyanoBacIdentification} introduces the web-based application for cyanobacteria detection, detailing the data acquisition process, deep learning approach, application deployment, preliminary results and conclusion. Finally, Section 4 concludes the report with a summary of key results and future directions for both projects.


\section{An analytical theory of convolutional neural network inverse problem solvers}\label{MMSE_analytical}
\noindent
\begin{minipage}[t]{0.48\linewidth}
\small
\vspace{0.4cm}
Convolutional Neural Networks (CNN) and other architectures~\citep{jin2017deep,mccann2017convolutional, zhu2018image} now achieve unprecedented performance in domains ranging from medical imaging to astrophysics and computer vision.
Yet, serious concerns have arisen regarding their reliability and robustness~\citep{antun2020instabilities}, with conflicting reports on whether small perturbations in the measurements or the forward operator lead to stable or unstable reconstructions~\citep{genzel2022solving}.
This work aims to provide a better understanding of the behavior of CNNs trained in a supervised manner for inverse problems. This is essential for defining the true domain of validity of such methods, assessing their stability, and enabling comparisons that go beyond empirical scoreboards. 
Our approach is based on the derivation of a closed-form MMSE estimator by incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality.
In the context of generative diffusion models, \citep{kamb2025an,scarvelisclosed} derive closed-form expressions for the MMSE under additive Gaussian noise (denoising) with architectural constraints such as equivariance and locality, and use them to study memorization and generalization. 
Locality has also been examined in~\citep{kadkhodaie2023learning} for modeling patch distributions.
For imaging inverse problems, equivariance has been widely explored in self-supervised learning~\citep{chen2023imaging,terris2024equivariant}.
In the supervised setting for linear inverse problems, it is standard to assume that a trained neural network approximates the MMSE~\citep{adler2018deep}. 
To the best of our knowledge, for general inverse problems, our work is the first to introduce additional functional constraints to more accurately characterize the action of neural networks and to derive closed-form expressions for the resulting constrained MMSE estimators. \todo{to reformulate}
\vspace{1cm}

My contribution to this work includes:
\begin{enumerate}
    \item Participating to derive theorical results
    \item implementing analytical expressions and optimizing numerical computations
    \item Training CNNs to validate to theorical results
    \item Participating to the writing of the scientific paper presenting this work.
\end{enumerate}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\begin{figure}[H]
\centering
\includegraphics[
  trim=30mm 0 0 0,
  clip,
  width=\linewidth
]{./tex_figure/teasor_v2_standalone.pdf}
\caption{
Our analytic theory accurately predicts neural network outputs across settings. We consider three inverse problems: denoising, inpainting, and deconvolution (left to right), on FFHQ, CIFAR10, and FashionMNIST datasets (top to bottom) with varying noise levels $\sigma$ (columns). For each setting, we show the measurements (top row), our analytic LE-MMSE estimator (second row), and outputs of trained UNet, ResNet, and PatchMLP models (last three rows). Theory closely matches network outputs.
}
\label{fig:teaser}
\end{figure}
\end{minipage}



\subsection{Problem formulation}
 Let $\vx \in \mathbb{R}^{N}$ is a random vector following an distribution $\pdata$, $A: \mathbb{R}^{N}\to \mathbb{R}^M$ a linear or non linear operator, and $\ve \sim \Normal{0, \sigma^2 \Id_M}$ a Gaussian noise independent of $\vx$. The measurement $\vy \in \mathbb{R}^M$ is given by the following forward model:
\begin{equation}
    \label{eq:forward}
    \vy = A \vx 
\end{equation}
We aim to construct an estimator $\hat{x}(\vy)$ estimating $\vx$ given $\vy$.
A modern approach is to use neural networks to solve this inverse problem, some are physics-aware, \ie incorporate the approximate inverse of $A$ in the architecture, while others are physics-agnostic, \ie do not require  any prior knowledge of $A$.
In this work, we focus on linear operators $A$ in the numerical experiments for didactic purposes, but our theoretical results hold for nonlinear operators $A$ as well.
We consider estimators of the form:
\begin{equation} \label{eq:estimator_shape}
    \hat{x}(\vy) = \neural(B \vy),
\end{equation}
where $\neural: \R^N \to \R^N$ is a neural network with weights $w \in \Theta$ with $\Theta$ a set of admissible weights, and $B \in \R^{N \times M}$ is a pre-inverse operator that can be chosen freely, in particular, the neural network can be physics-agnostic, \ie $B = \Id$ if $M=N$,  or physics-aware if $B$ is an approximate inverse of $A$ such as the transpose $A^\top$, the Moore-Penrose pseudoinverse $A^+$, or a regularized inverse.
They can be considered as a robust baseline, and be interpreted as a single-step variant of unrolled neural networks~\citep{adler2018learned,Celledoni_2021_equivariant_neural_networks}, which often achieve top performance in empirical benchmarks~\citep{muckley2021results}.


\paragraph{Empirical MMSE estimators}
We study neural networks $\phi^\star_\M$ trained as follows.
\begin{graybox}
\begin{definition}[Constrained empirical MMSE estimators\label{def:constrained_mmse}]
    Let $\D$ be a dataset of clean images. 
    We define the empirical MMSE estimators $\hat x_{\M} \eqdef \phi^\star_{\M} \circ B$, where % $\phi^\star_\M$ is the solution of
\begin{equation}\label{eq:constrained_MMSE}
    \phi^\star_\M \eqdef \argmin_{\phi \in \M} \; \frac{1}{2} \Mean{\norm{\phi (B \vy) - \vx}^2}.
\end{equation}
Here, $\vx$ follows the empirical distribution of the dataset $p_{\D} \!=\! \frac{1}{|\D|} \sum_{x \in \D} \delta_{x}$ and $\vy$ defined by~(\ref{eq:forward}). 
The set $\M$ encodes the range of functions reachable by the neural network,  $\M \!=\! \{\neural : w \in \Theta\}$, with $\Theta$ a set of admissible weights.    
\end{definition}    
\end{graybox}
Explicitly characterizing the set $\M$ given the neural network architecture $\neural$ is mathematically intractable.
Instead, we approximate $\M$ by considering larger functional classes that capture the architectural constraints of $\neural$ as follows.
\begin{definition}[MMSE, E-MMSE, LE-MMSE]
	\label{def:mmse}
    Our estimators of interest are of the form $\hat x = \phi^\star_\M\circ B$, where $\phi^\star_\M$ is the solution of~(\ref{eq:constrained_MMSE}).
    We define in~\cref{tab:summarize_mmse_def} these estimators with various choices of $\M$. 
    \begin{center}
        \begin{table}[H]
            \centering
            \vskip -0.1in
            \caption{Different constraint sets correspond to different variants of the MMSE estimator.}\label{tab:summarize_mmse_def}
            \vskip -0.1in
            \begin{small}
            % \resizebox{\columnwidth}{!}{
                \setlength{\tabcolsep}{30pt}
                \begin{sc}
                    \begin{tabular}{llll}
                    \toprule
                    & Constraint $\M$ & Estimator & See\\
                    \midrule
                    (1) & Any measurable func. & MMSE -- $\xmmse$ &(\ref{prop:expression_MMSE}) \\
                    (2) & (1) + Translation equiv. & E-MMSE -- $\xtrans$ &(\ref{thm:equivariant_mmse}) \\
                        & (2) + Locality & LE-MMSE -- $\xtransloc$ &(\ref{theorem:local_trans_estimator}) \\
                    \bottomrule
                    \end{tabular}
                \end{sc}
            % }
        \end{small}
        \end{table}
        \vskip -0.4in
    \end{center}
\end{definition}%
These function classes capture differents architectural constraints of the neural networks $\neural$.
According to the universal approximation theorem~\citep{hornik1989multilayer}, a sufficiently deep and large Multi-Layer Perception (MLP) can approximate any measurable function, hence the MMSE  $\xmmse$ serves as the theoretical limit for large-scale MLPs. 
The E-MMSE $\xtrans$ approximates CNNs without constraints on the receptive field, while the LE-MMSE $\xtransloc$ approximates the set of functions reachable by CNNs with finite receptive fields. 

\subsection{Unconstrainted MMSE estimator}
We etablish the closed-form expression of the unconstrained MMSE estimator $\xmmse$.
\begin{graybox}
    \begin{proposition}[Closed-form of the MMSE \label{prop:expression_MMSE}]
	The MMSE estimator in \Cref{def:mmse} writes, for any $y \in \Y$,
    \begin{equation}
        \xmmse(y) = \sum_{x\in\D} x \cdot w(x \vert y), \label{eq:empirical_mmse}  \\
    \end{equation}
    where $w(x \vert y) = \frac{\Normal{B y; B A x, \sigma^2 B B^{\top}}}{\sum_{x'\in \D} \Normal{B y; B A x', \sigma^2 B B^{\top}}}$.
    \end{proposition}
\end{graybox}

The estimator $\xmmse$ is a weighted average of the training examples, where the weights $w(x \vert y)$ are proportional to the likelihood of observing $x$ given $y$.
\begin{remark}
    When $B \!=\! \Id$, $\xmmse$ is exactly the \emph{posterior mean}: $\xmmse(\vy) = \Mean{\vx \vert \vy}$.
\end{remark}
\paragraph{Physics-aware estimator} 
To elucidate the role of $B$, we remark that the weights have the following explicit form:
\begin{equation*}%\label{eq:empirical_mmse_with_B} 
    w(x \vert y) \propto \exp \left( -\frac{1}{2\sigma^2}\norm{\Pi_{\Im{B^\top}}(A(\bar{x} - x) + e)}^2 \right)
\end{equation*}
By selecting $B=A^+$ or $A^\top$, we ensure that $\Im{B^T} = \Im{A}$, then inner term is $A(\bar{x} - x) + \Pi_{\Im{A}}e$. 
The noise $e$ is projected onto the image space of $A$, reducing its magnitude. This projection enhances the signal-to-noise ratio, justifying the fact that physics-aware estimators often outperform physics-agnostic ones.
However, beyond the projection effect, the choice of $B$ manifests no other roles: the resulting MMSE estimator remains identical as long as B satisfies the condition $\Im{B^T} = \Im{A}$. We discuss this further in~\cref{subsec:agnostic_vs_aware}.

\paragraph{Memorization of the empirical MMSE} 
The weighrs $w(x \vert y)$ is non-negative and sums to $1$ over $x \in \D$, hence $\xmmse(y)$ is a convex combination of the training images, \i.e $\xmmse(y) \in \mathrm{conv}(\D)$ -- the convex envelope of the dataset $\D$.
In particular, as $\sigma \to 0$, $\xmmse(y)$ converges to its nearest neighbor  $x in \D$, in the sense that $BAx$ is closest to $By$, tie being averaged.
The MMSE $\xmmse$ therefore \emph{memorizes} the dataset $\D$: it reconstructs by re-weighting the training examples and never extrapolates outside $\mathrm{conv}(\D)$.
\input{tex_figure/memorization.tex}
\subsection{Constrained MMSE estimators}\label{sec:analytical_formula}
We first provide details on the constraint classes and then describe our main theoretical results: analytical formulas and properties of constrained MMSE estimators.
\subsubsection{Functional constraints: equivariance and locality}\label{subsec:constraint_classes}
We focus on CNN inverse problem solvers by considering function classes $\M$ that encode two core inductive biases -- \emph{equivariance} to geometric transformations~\citep{cohen2016group,veeling2018rotation} and \emph{locality} via finite receptive field~\citep{kadkhodaie2023learning}. 

We start with formal definition of these concepts. 
Let $x \in \R^N$ represent an image defined on a discrete 2D grid of size $H \times W = N$.  
\begin{graybox}
\begin{definition}[Translation equivariant]\label{def:trans_equiv_fn}
Let $\T = \Z_{H} \times \Z_{W}$ be the group of 2D cyclic translations, where $\Z_H = \{0, 1, \ldots, H-1\}$ and $\Z_W = \{0, 1, \ldots, W-1\}$.
For each $g = (g_h, g_w) \in \T$, its action is represented by a permutation matrix $T_g \in \R^{N \times N}$, where $T_g x$ shifts the image $x \in \R^N$ by $g_h$ pixels vertically and $g_w$ pixels horizontally with periodic boundary conditions.
A measurable map $\phi: \X \to \X$ is said to be \emph{translation equivariant} if it satisfies $\phi(T_g x) = T_g \phi(x)$ for all $x \in \X$ and $g \in \T$.
We denote by $\Mtrans$ the set of all such maps.
\end{definition}
\end{graybox}
We now add locality constraint.
For each pixel $n$,
consider the patch extractor $\Pi_n : x \in \R^N \mapsto \Pi_n x = x[\omega_n] \in \R^P$ that extracts a square patch $x[\omega_n]$ of size $\sqrt{P} \times \sqrt{P}$ centered at pixel $n$, and $\omega_n$ is the set of pixel indices in the patch with circular boundary conditions.

\begin{graybox}
\begin{definition}[Local and translation equivariant]\label{def:transloc_equiv_fn}
    A measurable map $\phi: \X \to \X$ is \emph{local and translation equivariant} if there exists a measurable map $f: \R^P \to \R$ such that, for all $x \in \X$ and any pixel $n$, the output of $\phi$ at pixel $n$  denoted by $\phi(x)[n]$, depends only on the local patch $x[\omega_n]$, that is $\phi(x)[n] = f(\Pi_n x)$.
    We denote by $\Mtransloc$ the set of all such functions.
\end{definition}
\end{graybox}

The class $\Mtransloc$ captures standard CNNs with small kernels and weight sharing or patch-based models, \eg MLPs acting on patches~\citep{glimpse_local}.

\subsubsection{Preliminary remarks}
\paragraph{Constraints and projection} The constrained MMSE has a simple geometric interpretation.
\begin{proposition}\label{prop:projection_mmse}
    For a closed set $\M$, the $\M$-constrained MMSE estimator in~\cref{def:constrained_mmse} is the \textbf{orthogonal projection} in $L^2$ of the posterior mean $\Mean{\vx \vert \vy}$ onto the subspace of random vectors of the form $\mathcal{X} = \{ \phi(B\vy) : \phi \in \M \}$, that is $\hat{x}_{\M}(\vy) = \Pi_{\mathcal{X}} \left( \Mean{\vx \vert \vy} \right)$.
\end{proposition}
\paragraph{Architecture equivariance and data augmentation}
A standard approach to obtain equivariant estimators is by using data augmentation: the set $\D$ is replaced by $\T(\D) = \{T_g x : x \in \D,\, g \in \T\}$ at training time. 
This promotes \emph{reconstruction equivariance}~\citep{chen2021equivariant} of $\hat{x}_{\M}$:
\begin{equation}
    \label{eq:reconstruction_equivariance}
    \hat{x}_{\M}(A T_g \bar{x} + e) = T_g \hat{x}_{\M}(A \bar{x} + T_g^{-1} e).
\end{equation} 
The \emph{data-augmented} MMSE estimators, are defined below. 
\begin{graybox}
    \begin{definition}[Data-augmented MMSE estimator]\label{def:data_augmented_mmse}
        The data-augmented MMSE estimator $\hat{x}_{\M}^{\text{\tiny aug}}$ is defined as the MMSE estimator $\hat{x}_\M$ with respect to the empirical measure on $\T(\D)$. 
    \end{definition}
\end{graybox}
    Reconstruction equivariance is often a desired property, but it differs from the \emph{structural equivariance} of $\phi$~\citep{chen2020group,nordenforsdata}, as illustrated in~\cref{fig:illustration_translation}.


\subsubsection{Translation Equivariant MMSE estimator} 
We begin with the closed-form of the E-MMSE estimator $\xtrans$ and its properties. 
\begin{graybox}
    \begin{theorem}[Closed-form of E-MMSE]\label{thm:equivariant_mmse}
        The E-MMSE estimator writes, for any $y\in \R^M$
        \begin{equation}\label{eq:formula_equivariant_mmse}
            \xtrans(y) = \sum_{x \in \D, g \in \T} T_g x \cdot w_g(x \vert y),
        \end{equation}
        where $w_g(x \vert y) \propto \Normal{T_g^{-1} By; B A x, \sigma^2 BB^\top}$. 
        % and the normalization constant is such that $\sum_{x \in \D, g \in \T} w_g(x \vert y) = 1$.  
    \end{theorem}
    
\end{graybox}

\begin{remark}
    This result holds true for any group $\G$ of orthogonal transformations, not just translations.
\end{remark}
The normalization constant of the weights $w_g(x \vert y)$ ensures that they sum to $1$ over all $x \in \D$ and $g \in \T$, we ignore it here for brevity. 
The E-MMSE estimator $\xtrans$ is a weighted average of the augmented dataset $\T(\D)$. 
In particular, for denoising ($A=B=\Id$), they coincide. %, and it was already observed in~\citep{kamb2025an}.
However, depending on the forward operator $A$ and the pre-inverse $B$, it can differ significantly from $\xmmseaug$.
Some properties of the E-MMSE $\xtrans$ are presented in~\cref{cor:equivariant_mmse_properties} and illustrated in~\cref{fig:illustration_translation}.
\begin{corollary}\label{cor:equivariant_mmse_properties}
    % \begin{graybox}
    The E-MMSE estimator $\xtrans$ in~\cref{thm:equivariant_mmse} satisfies the following properties:
	\begin{itemize}[leftmargin=*,itemsep=.2em,topsep=0.pt]
        \item The E-MMSE estimator $\xtrans$ \emph{memorizes} the augmented dataset: reconstructed images live in $\mathrm{conv}(\T(\D))$.
        \item Data-augmentation and architecture equivariance are identical ($\xtrans = \xmmseaug$) if and only if $A$ and $B$ are circular convolution, with $B$ invertible. In this case, the E-MMSE is reconstruction equivariant (satisfies~(\ref{eq:reconstruction_equivariance})), physics-agnostic and physics-aware solvers are identical.
    \end{itemize}
    % \end{graybox}
\end{corollary}


\begin{figure}[ht]
    \centering 
    \vskip -0.1in
    \includegraphics[trim = 5mm 0 0 0, clip, width=\columnwidth]{./tex_figure/translation.pdf}
    \vskip -0.1in
    \caption{
        Architectural equivariance does not always guarantee reconstruction equivariance~(\ref{eq:reconstruction_equivariance}).
        Input $y$ in 2nd row is shifted. The E-MMSE estimator $\xtrans$ is reconstruction equivariant for deconvolution but not inpainting.
        The LE-MMSE estimator $\xtransloc$ is reconstruction equivariant for deconvolution and shows reduced sensitivity to shifts for inpainting. 
    }\label{fig:illustration_translation}
    \vskip -0.1in
\end{figure}

\subsubsection{Locality and Translation Equivariance}
We now analyze the effect of local receptive fields.
\begin{graybox}
\begin{theorem}[Closed-form of LE-MMSE]\label{theorem:local_trans_estimator}
    Suppose that the $N$ matrices $Q_{n} \eqdef \Pi_n B \in \R^{P \times M}$ have the \emph{same rank} $r>0$ for any $n$\footnote{This assumption can be relaxed: a general formula by rank stratification is provided in the complete paper.}.
    The LE-MMSE estimator $\xtransloc$ admits, for any $y \in \R^M$ the following expression, defined pixel-wise for each pixel $n'$:  
    \vskip -0.1in
    \begin{equation}\label{eq:loc_equiv_formula}
        \xtransloc(y)[n'] = \sum_{x \in \D} \sum_{n = 1}^{N} x[n] \cdot w_{n', n}(x \vert y),
    \end{equation}
    where $w_{n', n}(x \vert y) \propto \Normal{Q_{n'} y; Q_n A x, \sigma^2 Q_n Q_n^\top}$. 
    % and the normalization constant is such that $\sum_{x \in \D} \sum_{n = 1}^{N} w_{n', n}(x \vert y) = 1$. 
\end{theorem}
\end{graybox}

The value at pixel $n'$ of the LE-MMSE estimator is a weighted average of \emph{all} the pixels in the training images.
This recombination can produce images outside $\mathrm{conv}(\D)$ or $\mathrm{conv}(\T(\D))$ (contrary to the MMSE or E-MMSE); see~\cref{fig:memorization_illustration} and~\cref{sec:experiments}.
The weights $w_{n', n}(x \vert y)$ can be seen as the posterior probability of the pixel $x[n]$ given the patch $(B y)[\omega_{n'}] = Q_{n'} y$. 

% \todo{Edouard: il faudrait repréciser qui sont $x$, $\bar{x}$ et $e$}
Recall that $y = A \bar{x} + e$, with $\bar{x}$ the signal to recover and  
let $\Delta_{n',n}(\bar x, x) \!\eqdef\! (B A \bar x) [\omega_{n'}] \!-\! (B A x) [\omega_n]$, 
the weights can be rewritten as  $w_{n', n}(x \vert y) \!\propto\! \exp\left( - \frac{\eta^2}{2\sigma^2}\right)$ with:
\begin{equation}\label{eq:weights_signal_vs_noise}
    \eta \eqdef \norm{Q_n^+ \Delta_{n',n}(\bar x, x) + Q_n^+ Q_{n'} e}.
\end{equation}
Hence, the weights measure the similarity between \emph{local patches} of measured-then-reconstructed images with the metric $Q_n^+$. It is perturbed by the noise term $Q_n^+ Q_{n'} e$, which is responsible for the estimator's variance.
% \cref{theorem:local_trans_estimator} generalizes the result from~\cite{kamb2025an}, which was derived for denoising.  
Some properties of the LE-MMSE $\xtransloc$ are given in~\cref{cor:local_trans_estimator_properties}. 
% \begin{graybox}
    \begin{corollary}\label{cor:local_trans_estimator_properties}
        The LE-MMSE estimator $\xtransloc$ in~\cref{theorem:local_trans_estimator} satisfies the following properties:
		\begin{itemize}[leftmargin=*,itemsep=.2em,topsep=0.pt]
            \item The LE-MMSE estimator $\xtransloc$ is a \emph{patchwork} of training patches. As $\sigma\! \to \! 0$, each pixel is set to the central pixel of the best matching patch from $\D$.    
            % \item In general, architectural equivariance is different from data augmentation: $\xtransloc \neq \xtransloc^{\mathrm{\tiny aug}}$.
            \item It is not a posterior mean: there exists no prior distribution of $\vx$ such that $\xtransloc(y)\!=\!\Mean{\vx \vert \vy \!=\! y}$.
        \end{itemize}
    \end{corollary}
% \end{graybox}

\subsubsection{Physics-agnostic and physics-aware estimators}\label{subsec:agnostic_vs_aware}

The expectation of $\eta^2$ in~(\ref{eq:weights_signal_vs_noise}) is given by
\begin{equation*}
    \mathbb{E}_{\ve}\left[\eta^2\right] = \norm{Q_n^+ \Delta_{n',n}(\bar x, x)}^2 + \sigma^2 \cdot \Tr \left( \mathrm{Cov}_{n',n}\right)
\end{equation*}
with $\mathrm{Cov}_{n',n} = Q_n^+ Q_{n'} Q_{n'}^\top Q_n^{+\top}$.    
This expression reveals that the pre-inverse $B$ plays two distinct roles:

\begin{itemize}[leftmargin=*,itemsep=.2em,topsep=0.pt]
	\item \emph{Signal discrimination}: the term $Q_n^+ \Delta_{n',n}(\bar x, x)$ measures the amplification/reduction of difference between dissimilar/similar patches for the pre-inverse $B$. %amplifies differences between dissimilar patches while suppressing differences between similar patches. 
A good choice of $B$ should improve this discrimination.
	\item \ \emph{Noise robustness}: the second term $\Tr \left( \mathrm{Cov}_{n',n}\right)$ measures noise amplification/reduction for $B$. A good choice should reduce this amplification.
\end{itemize}

Ideally, one would like to choose $B$ to optimize both criteria, but unfortunately they can be conflicting: preserving the signal $BAx\approx x$ may amplify the noise, in relation to the classical bias-variance decomposition. Most importantly this effect is operator dependent, as we now discuss.

For inpainting, choosing a physics-aware estimator with $B=A^+$ is an effective way to reduce the noise sensitivity while keeping signal's information. Indeed, with this choice, the noise within the masked region is eliminated, while the signal is preserved in the unmasked region.
For deconvolution, however, the situation is different. Choosing $B=A^+$ can result in significant noise amplification (\ie $\Tr \left( \mathrm{Cov}_{n',n}\right)$ is large), which is not counter-balanced by a better signal's discrimination. In this setting, a regularized inverse, balancing both aspects should likely be preferred.
This effect is illustrated in~\cref{fig:aware_vs_agnostic}.
When designing reconstruction algorithms, this trade-off between data-consistency and noise amplification must be carefully considered, in relation to the inverse problem at hand. 

\begin{figure}[ht]
    \centering
    \includegraphics[trim=8mm 0 0 0, clip,width=\columnwidth]{./tex_figure/aware_vs_agnostic.pdf}
    \vskip -0.1in
    \caption{
    %The operator $B$ should be chosen carefully to balance noise amplification and signal's discrimination trade-off.
	Physics-aware ($B\!=\!A^+$, bottom) has lower variance for inpainting (left), while physics-agnostic ($B\!=\!\Id$, top) has lower variance for deconvolution (right).
    Mean and pixel-wise variance are computed w.r.t $50$ noise realizations.         
    }
    \label{fig:aware_vs_agnostic}
    \vskip -0.15in
\end{figure}


\subsection{Numerical Experiments}\label{sec:experiments}
While the MMSE and E-MMSE estimators are interesting from a theoretical perspective, the LE-MMSE estimator is more relevant to practical neural network architectures used in imaging inverse problems.
We validate our theoretical findings about the LE-MMSE on three representative inverse problems: denoising, inpainting, and deconvolution. 
%\todo{We focus on LE-MMSE because \dots}
%{\color{blue} We focus on the LE-MMSE because it corresponds to one of the most widely used architectures in imaging inverse problems: CNNs with small kernels and weight sharing.}
\subsubsection{Experimental setup}    
    We implement three different local and translation equivariant neural network architectures: UNet2D~\cite{ronneberger2015u}, ResNet~\cite{he2016deep} and PatchMLP (an MLP acting on patches).
    % ; see~\cref{sec:supp_numerical} for architecture and training details.

    Implementing the analytical formula in~\cref{theorem:local_trans_estimator} requires computing a huge amount of pairwise distances, which is computationally demanding. We therefore restrict our experiments to $32\times 32$ color images and datasets of $10^4$ images. To accumulate Gaussian weights robustly and stably across batches, we use a careful online log-sum-exp implementation of the weighted averages (numerator and denominator). %, see~\cref{code:supp_python_code_locequiv_mmse,code:supp_online_sum_exp}. 
    % \todo{Je ne suis pas sûr que ce soit une bonne idée. J'aime bien personnellement, mais je trouve le relecteur moyen bête et méchant maintenant :). Edouard: Idem, on pourrait juste garder la première phrase et dire qu'on détaille les astuces d'implémentation dans l'appendice.}
    % \todo{Hai: Je les ai mis là au cas où, on peut les supprimer aussi. Moi je trouve que ça montre qu'on a bien implémenté la formule :)}

    For inpainting we use a square mask of size $15 \times 15$ at the center. For deconvolution, we use an isotropic Gaussian kernel of standard deviation of $1.0$.
    The noise level $\sigma$ is varying uniformly between $0$ and $1.0$ during training.


\subsubsection{Analytical formula and neural networks}\label{sec:neural_vs_analytical}
\paragraph{Case-by-case neural outputs prediction}
    We verify that trained networks approximate the LE-MMSE estimator, by comparing their outputs to the formula in~\cref{theorem:local_trans_estimator}.
    The PSNR between the trained UNet2D, the formula and the ground truth are reported in~\cref{fig:neural_vs_analytical_unet2d_patch_5}. 
    The reconstruction quality (orange and blue curves) degrades as noise level increases, which is expected as noise makes the problem harder.
    However, the PSNR between neural networks and the analytical formula (green curves) remains high ($\gtrsim 25$ dB) across all noise levels, indicating a strong alignment between the two.
    Similar behavior is observed in~\cref{tab:neural_vs_analytical_psnr} for other architectures and datasets. 
    \begin{figure}[ht]
        \centering
        \def\base{./images/neural_vs_analytical/localequivunet2dcondmodel_patch_5/plots}
        \vskip -0.05in
        \includegraphics[width=\linewidth]{\base/FFHQ_images32x32_subset_10000_combined_psnr_grid.pdf}
        \vskip -0.1in
        \caption{The green curves reveals a strong agreement (PSNR $\gtrsim\!25$ dB) between the trained UNet2D and the analytical formula for all inverse problems. 
        Median and interquartile range (IQR) using $50$ images per $\sigma$, $P = 5\times 5$ and $B\!=\!\Id$.
        }
        \label{fig:neural_vs_analytical_unet2d_patch_5} 
        \vskip -0.1in
    \end{figure}%
    We provide extensive numerical results on various settings (architectures, datasets, tasks, physics-agnostic/aware, patch sizes) in the complete paper. 
    These results confirm that \emph{trained neural networks closely approximate the analytical LE-MMSE estimator}.
    It is remarkable that different architectures consistently yield a similar output, as shown in~\cref{fig:teaser}. 
        
    \begin{table*}[ht]
        \centering
        \caption{
        \cref{theorem:local_trans_estimator} is verified across architectures, tasks, and datasets.
        We show the median of the PSNR between neural networks and the analytical formula with $P = 5 \times 5$ and $B\!=\!\Id$.
        The PSNR on FashionMNIST is consistently higher ($2\sim 3$ dB) than on FFHQ and CIFAR10, which we attribute to the lower complexity of FashionMNIST images.
        A lower PSNR on the test set in low-noise regimes is explained by the low-density regions.
        }
        \label{tab:neural_vs_analytical_psnr}
        \vspace*{-0.1cm}
        \includegraphics[width=.85\textwidth]{./images/neural_vs_analytical/tab_neural_vs_analytical_psnr_patch_5_side-by-side.pdf}
        \vskip -0.1in
    \end{table*}

\paragraph{Low-density regions}\label{sec:neural_generalization}
    A noticeable train-test gap remains in~\cref{fig:neural_vs_analytical_unet2d_patch_5,tab:neural_vs_analytical_psnr}, most pronounced at low noise level. 
    We attribute this to the \emph{measurement distribution} $p(y)$ induced by the empirical distribution.
    It is a Gaussian mixture, centered at the training measurements $Ax$: $p(y) = \frac{1}{|\D|} \sum_{x\in \D} \Normal{y; Ax, \sigma^2 \Id_M}$. 
    Its density is high near the centers $A x, x \in \D$, or for large noise levels $\sigma$, creating overlap between Gaussians. It becomes negligible far from the centers in low noise regimes.
    \begin{figure}[ht!]
        \centering
        \vskip -0.1in
        % \includegraphics[width=\columnwidth]{./images/neural_generalization/localequivunet2dcondmodel_patch_5/plots/multiop_FFHQ_images32x32_top5000_sigma_0.1_subset_10000_neg_log_density_psnr_identity_inform.pdf}
        \includegraphics[width=\columnwidth]{./images/neural_generalization/localequivunet2dcondmodel_patch_5/plots/multiop_FFHQ_images32x32_top5000_sigma_0.05_subset_10000_neg_log_density_psnr_identity_inform.pdf}
        \vskip -0.1in
        \caption{
            Higher density yield better alignment.
            We select test images from FFHQ-32, compute $-\!\log p(y)$ and plot it against the PSNR between the outputs of the LE-MMSE formula and a trained UNet2D, $\sigma=0.05$ and $P = 5 \times 5$. 
        }
        \label{fig:neural_generalization_unet2d}
        \vskip -0.1in
    \end{figure}%

    Both LE-MMSE and the networks are optimized with respect to the measurement distribution $p(y)$.   
    The networks and the theoretical formula best align in high-density regions of $p(y)$.
    This phenomenon is illustrated in~\cref{fig:neural_generalization_unet2d}, where we show that the alignment degrades as $-\!\log p(y)$ increases (or equivalently as $p(y)$ decreases). 
    Note that here we select 5000 test images with equally spaced $-\!\log p(y)$ values to cover a wide range of densities. 
    For large $\sigma$, the effect is less pronounced as the Gaussians overlap more.

    \paragraph{Out-of-distribution: how do networks generalize?}
    Neural networks often generalize well to out-of-distribution (OOD) data, but understanding this behavior remains an open question.  
    Interestingly, our analytical formulas provide insights to explain this phenomenon. 
    We consider here images $\bar{x}$ that lie in another dataset $\D'$ disjoint from the training set $\D$.  
    \begin{figure}[ht!]
        \centering
        \includegraphics[trim=17mm 0 0 0,clip,width=\linewidth]{./tex_figure/fig_qualitative_ood_unet2d_patch_5_ffhq.pdf}
        \caption{Our theory can predict the neural network outputs for out-of-distribution data: both UNet2D and the LE-MMSE formula are trained (or computed) on $\D = $ FFHQ-32. 
        They are tested on $\D' = $ CIFAR10.}
        \label{fig:ood_unet2d_patch_5}
        \vskip -0.1in
    \end{figure}
    In~\cref{fig:ood_unet2d_patch_5}, we compare the output of UNet2D and the LE-MMSE formula.
    %  on CIFAR10 images, while both are trained/computed on FFHQ-32.
    For large $\sigma$, we observe very close outputs between the neural network and the analytical formula.
    For small $\sigma$, as the OOD images lie in low-density regions of $p(y)$, the alignment degrades as expected from the previous discussion.
    However, even in this regime, the PSNR between the two outputs remains reasonably high.  
    This suggests that the network approximates the LE-MMSE estimator even on OOD data: generalization of the neural networks can be understood through the lens of the LE-MMSE estimator -- it leverages local patches from the training set to reconstruct unseen images. 


\subsubsection{Smoothed LE-MMSE and spectral bias}\label{subsec:smoothed_formula}
Deep neural networks also exhibit a \emph{spectral bias}~\citep{xu2019frequency,rahaman2019spectral}, \ie a preference for low-frequency functions, which yields smoother reconstructions in practice. 
Describing this spectral bias with functional constraints is missing from our derivation.
We model this effect by considering a \emph{randomized smoothing}~\citep{duchi2012randomized} variant: 
for a smoothing parameter $\epsilon > 0$ and $\vz \sim \Normal{0, \Id_M}$, define 
$\xtransloc^{\mathrm{sm}}(y) \eqdef \Mean{\xtrans(y + \epsilon \cdot \vz)}$. 
This averages the estimates over random perturbations, acting as a nonlinear low-pass filter that attenuates high-frequency variability in the output. 
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=\columnwidth]{images/comparison_smoothed_vs_original/FFHQ_images32x32_subset_10000_sigma_0.05_patch_5_comparison_box.pdf}
    \vskip -0.1in
    \caption{
        Smoothed LE-MMSE ($\epsilon=0.05$) better matches neural network and improves reconstruction quality.}
        \label{fig:smoothed_vs_original}
    \vskip -0.1in
\end{figure}
It also mirrors standard neural network training, where different noise realizations are seen across epochs.
In~\cref{fig:smoothed_vs_original}, we compare the smoothed estimator $\xtransloc^{\mathrm{sm}}$ with the original $\xtransloc$, both computed with $P = 5 \times 5, \sigma=0.05$ and $B = \Id$ on FFHQ-32 images. 
We observe that the smoothed LE-MMSE better matches the neural network output ($1\sim3$ dB) and improves reconstruction quality ($\sim 1$dB).
This suggests that we can better capture neural network behavior by incorporating carefully designed smoothness constraints.

\subsection{Conclusion}
Through the lens of MMSE estimator in the context of imaging inverse problems, our work provides a theoretical framework to analyze the CNNs.
The strongly quantitative and qualitative agreement between our theoretical formulas and trained neural networks validates our LE-MMSE estimator. 
While being tractable and interpretable, our derived closed-form LE-MMSE offers valuable insights into the behavior of these networks under various settings: (1)the role of the foward operator $A$ and the pre-inverse $B$ on the resconstruction, (2)memorization does not contrardict generalization, i.e how these networks can extrapolate to out-of-distribution data by memorizing the local patches from the training set.
Our analysis also leverages the strength of neural networks in general by highlighting their ability to compress large datasets into their considerably smaller number of parameters, then rearticulate efficiently these parameters to approximate closely the heavily computational LE-MMSE estimator enabling fast and low-computational inference, explaining their practical success.
These findings open new avenues for further research, such as incorporating additional constraints to model other neural network such as smothness constrainsts to capture spectral bias, or permutation equivariance to model transformers~\citep{xu2024permutation}. The Gaussian assumption can be relaxed (e.g., exponential-family) by using the appropriate likelihood $p_{\vy \vert \vx}(y \vert x)$. 
Finally, beyond MSE, practical training often uses $\ell_1$ (yielding posterior median), perceptual, or mixed losses.
Analyzing the MMSE analogs under these alternatives is an interesting future venue.




\newpage
\section{Cyanobacteria recognition using deep learning techniques}\label{cyanoBacIdentification}
Cyanobacteria are essential components of aquatic ecosystems~\citep{singh2016cyanobacteria,diez2014ecological,saleem2025cyanobacteria}, driving global biogeochemical cycles through processes such as nitrogen fixation and oxygen production and carbon circulation. 
Their monitoring is critical for monitoring water quality, detecting harmful algal blooms, and assessing ecosystem dynamics.
However, the primary bottleneck in this process remains the rapid and accurate taxonomic identification of these organisms.
Traditional identification methods rely heavily on manual microscopic examination by experts, which is time-consuming, labor-intensive, subjective, and often impractical for large-scale monitoring efforts.
Various deep learning techniques have been proposed to automate the identification process, some focus with microscope image with a limited number of cyanobacteria species or genera \citep{blanco2025multimodal,baek2020identification}. Others focus on satellite images~\citep{kutser2004quantitative}, synthetic images~\citep{barrientos2023semantic} to detect cyanobacteria blooms.
To the best of our knowledge, there is no existing web-based application capable of identifying a wide taxonomic range of cyanobacteria from microscope images using deep learning.
To address this gap, we introduce CyDect (Cyanobacteria Detection), a web-based platform capable of detecting and classifying over \todo{number of classes} distinct cyanobacteria genera from microscope images. 
CyDect utilizes a Faster Region-based Convolutional Neural Network (Faster R-CNN) to handle the significant morphological variability of these organisms. 
The platform features a dual-interface architecture designed to bridge the gap between expert research and citizen science: it provides advanced analysis tools for biologists while simultaneously offering an accessible portal for the general public to upload images and visualize detection results.
\subsection{Data acquisition}
The dataset consists of microscope images of water samples containing cyanobacterias, these images are captured using high-resolution microscopes by our collaborators, biologists specialized in cyanobacteria study. To enhance the model robustness and generalization ability, the dataset includes images taken under various conditions, such as different lighting, magnification levels, and sample preparations. Each image is annotated with bounding boxes around individual cyanobacteria along with their corresponding species labels by our biologist experts. Initially, the dataset contains over \todo{number of images} images covering more than \todo{number of classes} different cyanobacteria species, but the dataset is continously enriched with new images and annotations as more samples are collected and processed.

\todo{Add more detail about the dataset}.

\subsection{Deep learning approach}
\subsubsection{Overview of RCNN family}
Faster-RCNN is a variant of the RCNN, which are a family of machine learning model for computer vision, and specifically object detection and localization. A RCNN model takes an image as input and then output bouding boxes around the detected objects along with their predicted class labels. In particular, a RCNN model taskes an input image, extracts region proposals that are likely to contain objects using Seletive Search algorithm \citep{uijlings2013selective}, computes features for each proposal using a learge convolutional neural network (CNN), then and then classifies each region using class-specific linear SVMSs \citep{girshick2014richfeaturehierarchiesaccurate}, as shown in Figure \ref{fig:rcnn_demo}. Originally, RCNN encounter several inefficiencies, one of these is training RCNN models involves multiple stages, including pre-training the CNN on a large dataset, fine-tuning it on the target dataset, and training class-specific SVMs, which makes the training process complex and time-consuming. Moreover, RCNN requires significant storage space as it needs to store the features for each region proposal, the model also suffers from botllenect as each region proposal need to be processed independently by the CNN, selective search for region proposals is also slow and not computationally expensive. These limitations are addressed by the fast-RCNN and faster-RCNN models which are presented below.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/RCNN_demonstration.png}
    \caption{RCNN demonstration. From \textit{Convolutional Neural Networks for Computer Vision} [Lecture notes] by Juliette Chevallier, 2025, INSA Toulouse.}
    \label{fig:rcnn_demo}
\end{figure}

Fast-RCNN response to this inefficiency by (1) performing feature extraction over the image before proposing regions, and (2) replaceing the SVM with a softmax layer \citep{girshick2015fastrcnn}. The first improvement is made by passing the entire image through an large CNN to generate feature maps, then region proposals are projected onto these feature maps as shown in Figure \ref{fig:fastrcnn_demo}. With these two improvements, fast-RCNN avoid the bottleneck as feature extraction is shared across region proposals, and a fast-RCNN is a unified model as the classifier is now part of the neural network, leading to a significant reduction in training and inference time.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/FastRCNN_demonstration.png}
    \caption{Fast-RCNN demonstration. From \textit{Convolutional Neural Networks for Computer Vision} [Lecture notes] by Juliette Chevallier, 2025, INSA Toulouse.}
    \label{fig:fastrcnn_demo}
\end{figure}


However, Fast-RCNN still rely on selective search for region proposals, which is not computationally efficient. Faster-RCNN solve this issue by replacing slow selective search with a region proposal network (RPN), which is a fully convolutional network that shares the convolutional layers with the object detection network \citep{ren2016fasterrcnnrealtimeobject}. The RPN takes the feature maps generated by the shared convolutional layers as input and outputs a set bounding box coordinates, each with an objectness score indicating the likelihood of containing an object, if this score is above a certain threshold, i.e the corresponding box has a good objectness, its coordinate get passed forward as a region proposal. The RPN uses a sliding window approach to generate boxes at different scales with certain fixed ratios, called anchor boxes, as shown in Figure \ref{fig:anchor_box}. By sharing the convolutional layers between the RPN and the object detection network, faster-RCNN significantly reduces the computational cost of region proposal generation, leading to faster inference times. The general architecture of a faster-RCNN is shown in Figure \ref{fig:fasterrcnn_demo}. 

\subsubsection{Faster-RCNN for cyanobacteria identification}
At the day this paper is written, the faster-RCNN model is no longer the state-of-the-art for object detection tasks, however, we still choose this model for this cyanobacteria identification task due to its robustness and high accuracy for small object detection, which is crucial as cyanobacterias are often small and densely packed in microscope images. Moreover, training faster-RCNN is easier compared to more recent models such as YOLO-family models and transformer-based models, which often require more complex training procedures, hyperparameter tuning and larger datasets to achieve good performance. To enhance the performance of our faster-RCNN model, we employ the ResNet (Residual Network) architecture integrating with Feature Pyramid Network (FPN) technique as the backbone network for feature extraction. This combination allows us to improve the model's ability to detect cyanobacterias at different scales, as we show now.



\begin{figure}[ht!]
    \centering
    % First Subfigure
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/anchor_boxes.png}
        \caption{Anchor boxes used in the region proposal network \citep{ren2016fasterrcnnrealtimeobject}.}
        \label{fig:anchor_box}
    \end{subfigure}
    \hfill % Adds flexible space between images
    % Second Subfigure
    \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/fasterrcnn_demonstration.png}
        \caption{Faster-RCNN demonstration. From \textit{Convolutional Neural Networks for Computer Vision} [Lecture notes] by Juliette Chevallier, 2025, INSA Toulouse.}
        \label{fig:fasterrcnn_demo}
    \end{subfigure}
    \caption{Anchor boxes and Faster-RCNN demonstration.}
    \label{fig:both_images}
\end{figure}

\paragraph{ResNet architecture}
ResNet (Residual Network) is a deep convolutional neural network architecture that introduces the concept of residual learning by stacking residual blocks.
The residual blocks learn residual functions with reference to the layer inputs, instead of learning unreferenced functions by connecting the input of a layer directly to its output via shortcut connections.
Formally, let $\mathbf{x}$ be the input to a certain layer, and let $\mathcal{F}(\mathbf{x})$ be the desired underlying mapping to be learned by the layer.
Instead of directly approximating $\mathcal{F}(\mathbf{x})$, residual blocks reformulate the mapping as $\mathcal{F}(\mathbf{x}) = \mathcal{H}(\mathbf{x}) - \mathbf{x}$, where $\mathcal{H}(\mathbf{x})$ is the original mapping.
The layer then learns the residual function $\mathcal{F}(\mathbf{x})$, and the output of the layer becomes $\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x}) + \mathbf{x}$, as illustrated in Figure \ref{fig:residual_block}.
This reformulation allows the network to learn identity mappings more easily, which helps to mitigate the vanishing gradient problem and enables the training of much deeper networks \citep{he2015deepresiduallearningimage}, often leading to improved performance on various computer vision tasks. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/residual_block.png}
    \caption{An example of residual block in ResNet architecture \citep{he2015deepresiduallearningimage}.}
    \label{fig:residual_block}
\end{figure}

\paragraph{Feature Pyramid Network (FPN)}
Feature Pyramid Network (FPN) is a multi-scale feature extraction technique that enhances the ability of convolutional neural networks to detect objects at different scales. FPN constructs a feature pyramid by combining low-resolution, semantically strong features with high-resolution, semantically weak features through a top-down pathway and lateral connections \citep{lin2017featurepyramidnetworksobject}, as illustrated in Figure \ref{fig:fpn_architecture}. This approach allows the network to leverage both high-level semantic information and fine-grained spatial details, which is particularly beneficial for detecting objects with different scales like cyanobacterias in microscope images with varying magnifications.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fpn_architecture.png}
    \caption{Feature Pyramid Network \citep{lin2017featurepyramidnetworksobject}: blue outlines denote feature maps, with thicker lines indicating stronger semantic features. A top-down pathway upsamples high-level features and combines them with corresponding lower-level maps through lateral connections, generating rich multi-scale representations.}
    \label{fig:fpn_architecture}
\end{figure}

\paragraph{Combining ResNet and FPN to form the backbone of Faster-RCNN}
We integrate the ResNet architecture with FPN to form the backbone of our faster-RCNN model.
The ResNet layers extract hierarchical features from the input images, while the FPN enhances these features by creating a multi-scale feature pyramid. 
This combination allows the faster-RCNN model to effectively detect cyanobacterias of various sizes and scales in microscope images, improving overall detection accuracy and robustness.
The logical Resnet-FPN backbone architecture is described as following: the input image is first feeded into the Resnet to extract feature maps at different levels, then these feature maps are passed through the FPN to generate a set of multi-scale feature maps, as shown in Figure~\ref{fig:fpn_architecture}. 
The anchor boxes in the RPN are then applied to these multi-scale feature maps in such a way that each level of the feature pyramid is responsible for detecting objects of specific scales. In particular, anchor boxes of a single scale are assigned to each level of the feature pyramid, since each level corresponds to a different spatial resolution, allowing the model to effectively capture objects of varying sizes. The overall architecture of the ResNet-FPN backbone within the faster-RCNN framework is illustrated in Figure~\ref{fig:resnet_fpn_combining}. 
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/resnet_fpn.png}
    \caption{ResNet-FPN backbone within the Faster-RCNN framework: the input image is first processed by the ResNet to extract feature maps at different levels, then these feature maps are passed through the FPN to generate a set of multi-scale feature maps, which are then used by the RPN and the detection head for object detection.}
    \label{fig:resnet_fpn_combining}
\end{figure}

\subsection{Application deployment}
Our application is desgined to serve two distinct user groups: experts (biologists, researchers, taxonomists) and the general public. It consists of three main components: the expert interface, the general public interface, and the model backend API, all these components are hosted on a cloud server to ensure accessibility and scalability.
These elements are described in detail below.


\paragraph{API backend server}
The API backend server is built using \href{https://flask.palletsprojects.com/en/stable/}{Flask}. The server hosts the trained faster-RCNN model and the model training scripts. 
When users request predictions, the corresponding images are sent to the API backend server via an HTTP POST request, The server processes the image using the faster-RCNN model, generates detection results, and sends them back to the user interface in JSON format for display.

\paragraph{General user interface} The general user interface is developed using \href{https://dash.plotly.com/}{Plotly Dash}, a popular framework for building interactive web applications in Python.
The interface is desgined to be free-access, user-friendly and does not require users to have any technical background.
It allows users to upload microscope images and view the detection results generated by the faster-RCNN model. Moreover, users can provide feedback on the detection results by correcting misidentified cyanobacteria or adding missing annotations. This feedback is collected and stored in a database, waiting for expert validation to ensure the quality and accuracy of the annotations.\todo{Add image of this interface}


\paragraph{Expert interface} We use \href{https://labelstud.io/}{Label Studio}, an open-source data labeling tool, for the expert interface. The access to this interface is restricted to authorized experts only.
Experts can upload, annotate new microscope images, validate user corrections from the general public interface, and manage the dataset effectively.
The annnotation process is facilitated by enabling pre-annotations using the current faster-RCNN model, allowing experts to review and adjust the model's predictions rather than starting from scratch.
Additionally, they can also trigger the model retraining process, ensuring that the backend model remains up-to-date with the latest annotated data and continuously improves its performance.\todo{Add image of this interface} 

\paragraph{Hosting server} The application is hosted on a cloud server provided by \href{https://drocc.fr/crocc/}{Cloud Recherche Occitanie (CROCC)}.

In general, the application workflow is as follows: users upload microscope images through either the expert interface or the general public interface, these images are then sent to the API backend server where the faster-RCNN model process them and return the detection results to the user interfaces for display; experts can also manage and enrich the dataset by uploading their own microscope images or/and validating user-correction; the model retraining proccess can only be triggered by experts, as shown in Figure~\ref{fig:app_workflow}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/app_workflow.png}
    \caption{Application workflow diagram: users upload microscope images through either the expert interface or the general public interface, these images are then sent to the API backend server where the faster-RCNN model process them and return the detection results to the user interfaces for display; experts can also manage and enrich the dataset by uploading their own microscope images or/and validating user-correction; the model retraining proccess can only be triggered by experts.}
    \label{fig:app_workflow}
\end{figure}
\subsection{Preliminary results}
\subsection{Conclusion and future work}


\newpage
\bibliography{biblio.bib}


\newpage
\appendix
\onecolumn
{\LARGE{Annexes for the first project -- An analytical theory ofconvolutional neural networks in imaging inverse problems}}
\input{appendix.tex}


\end{document}