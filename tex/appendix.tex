\section{Mathematical formalism}
\subsection{Preliminaries}\label{sec:supp_preliminaries}
\paragraph{Notation conventions}
In what follows, we use the following notation:
\begin{itemize}
    \item Sets are indicated by calligraphic letters, \eg $\D, \M, \T, \G$.
    \item Random vectors are denoted by bold lowercase letters, \eg $\vx, \vy, \vz$.
    \item $A \in \R^{M \times N}$ denotes a linear operator from $\R^N$ to $\R^M$ (\eg convolution, inpainting).
    \item $\Normal{x; \mu, \Sigma}$ denotes the probability density function (PDF) of a Gaussian distribution with mean $\mu$ and covariance $\Sigma$ evaluated at $x$. The covariance matrix $\Sigma$ can be non-singular or singular (see \cref{def:supp_degenerate_gaussian}). 
    \item The $n$-th coordinate of a vector $x\in \R^N$ is denoted either $x_n$ or $x[n]$. Similarly, the coordinates of a multivalued function $\phi:\R^N\to \R^N$  can be denoted either $\phi_n(x)$ or $\phi(x)[n]$.
    \item The empirical data distribution $\ptrain$ is defined by 
    \begin{equation}
        \ptrain = \frac{1}{|\D|} \sum_{x\in \D} \delta_{x},   
    \end{equation} 
    where $\D$ is the training dataset of finite size $|\D|$.
\end{itemize}

\paragraph{Degenerate Gaussian distribution}
We can define the multivariate Gaussian distribution for positive semi-definite covariance
matrices~\citep{rao1973linear}. Below, we recall a simple definition of the positive semi-definite covariance 
multivariate Gaussian, sometimes called the degenerate or singular multivariate Gaussian.
\begin{definition}[Degenerate Gaussian distribution]\label{def:supp_degenerate_gaussian}
    A random vector $\vz \in \R^N$ has a Gaussian distribution with mean $\mu \in \R^N$ and covariance matrix $\Sigma \in \R^{N \times N}, \Sigma \succeq 0$ if its probability density function (PDF) is given by:
    \begin{equation}
        \Normal{z; \mu, \Sigma} = \frac{1}{(2\pi)^{r/2} \sqrt{|\Sigma|_{+}}} \exp\left(-\frac{1}{2} (z - \mu)^\top \Sigma^{+} (z - \mu)\right) \mathbbm{1}_{\supp{\vz}}(z),
    \end{equation}
    on the support $z \in \supp{\vz} = \mu + \Im{\Sigma}$ and zero elsewhere.  
    In this equation, $r = \mathrm{rank}(\Sigma)$, $\Sigma^{+}$ denotes the Moore-Penrose pseudo-inverse of $\Sigma$ and $|\Sigma|_{+}$ is the pseudo-determinant of $\Sigma$ (product of non-zero eigenvalues). 
\end{definition}
The support $\supp{\vz}$ of the distribution is the $r-$ dimensional affine subspace of $\R^N$, $r$ is the rank of $\Sigma$. The density is with respect to the Lebesgue measure restricted to $\supp{\vz}$, constructed via the pushforward measure of the standard Lebesgue measure on $\R^r$ by the affine map $v \mapsto \mu + \Sigma_r v$, where $\Sigma = \Sigma_r \Sigma_r^\top$.  
This measure also coincides (up to a normalization constant, equal to the pseudo-determinant of $\Sigma$) with the $r-$dimensional Hausdorff measure. 
For completeness, we provide a derivation of the density of $\vz$ with respect to the $r$-dimensional Hausdorff measure $\mathcal{H}^r$ on the affine support $\supp{\vz}$.

    Suppose $\Sigma_r \in \mathbb{R}^{n \times r}$ satisfies $ \Sigma = \Sigma_r \Sigma_r^\top $ and $\mathrm{rank}(\Sigma) = r$.
    Define the random vector $\vz = \mu + \Sigma_r \vw$, where $\vw \sim \mathcal{N}(0, I_r)$.
    Consider the affine map $ \Phi: \mathbb{R}^r \to \mathbb{R}^N $ defined by $ \Phi(w) = \mu + \Sigma_r w $.
    This map is a smooth bijection from $\mathbb{R}^r$ onto the support $\supp{\vz} = \mu + \Im{\Sigma}$.

    The probability measure of $\vz$, denoted $\mathbb{P}_{\vz}$, is the pushforward of the standard Gaussian measure $\gamma^r$ via $\Phi$. The standard Gaussian density on $\mathbb{R}^r$ is $g(w) = (2\pi)^{-r/2} e^{-\frac{1}{2} \|w\|^2}$.
    For any measurable set $A \subset \supp{\vz}$, we have:
    \begin{equation}
        \mathbb{P}_{\vz}(A) = \gamma^r(\Phi^{-1}(A)) = \int_{\Phi^{-1}(A)} g(w) \, d\lambda^r(w).
    \end{equation}
    To find the density with respect to the Hausdorff measure $\mathcal{H}^r$ (the standard volume measure on the support), we apply the Area Formula~\citep{federer1996geometric} (change of variables). The Jacobian determinant of $\Phi$ is:
    $$ J\Phi = \sqrt{\det(\Sigma_r^\top \Sigma_r)} = \sqrt{|\Sigma|_+}. $$
    Using the change of variables $z = \Phi(w)$, the volume elements relate via $d\mathcal{H}^r(z) = J\Phi \, d\lambda^r(w)$. Therefore:
    $$ d\lambda^r(w) = \frac{1}{\sqrt{|\Sigma|_+}} d\mathcal{H}^r(z). $$
    Substituting this into the integral and using $w = \Phi^{-1}(z) = \Sigma_r^+ (z - \mu)$:
    \begin{align*}
        \mathbb{P}_{\vz}(A) &= \int_{A} g(\Phi^{-1}(z)) \frac{1}{\sqrt{|\Sigma|_+}} d\mathcal{H}^r(z) \\
        &= \int_{A} \frac{1}{(2\pi)^{r/2}\sqrt{|\Sigma|_+}} \exp\left( -\frac{1}{2} \|\Sigma_r^+ (z - \mu)\|^2 \right) d\mathcal{H}^r(z).
    \end{align*}
    Noting that $\|\Sigma_r^+ (z - \mu)\|^2 = (z - \mu)^\top (\Sigma_r^+)^\top \Sigma_r^+ (z - \mu) = (z - \mu)^\top \Sigma^+ (z - \mu)$, we recover the density given in Definition \ref{def:supp_degenerate_gaussian}.

When the covariance matrix is non-singular, we recover the standard PDF of a Gaussian distribution. 

\paragraph{Degenerate Multivariate Gaussian and Mahalanobis Distance}
The Mahalanobis distance quantifies the distance from $z$ to $\mu$ relative to the covariance structure and is defined as:

\begin{equation}
D_\Sigma(z, \mu) = \sqrt{(z - \mu)^\top \Sigma^+ (z - \mu)}.
\end{equation}

When $z \not\in \supp{\vz}$, the density is zero, corresponding to an infinite effective distance outside the support.

\begin{itemize}
    \item Eigenvalue Decomposition. Consider the spectral decomposition of the covariance matrix:
    \begin{equation*}
        \Sigma = U \Lambda U^\top,
    \end{equation*}
    where:
    \begin{itemize}
        \item $U$ is an orthogonal matrix whose columns are the eigenvectors of $\Sigma$.
        \item $\Lambda = \diag(\lambda_1, \lambda_2, \dots, \lambda_d)$ is a diagonal matrix with eigenvalues $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_r > 0 = \lambda_{r+1} = \dots = \lambda_d$, sorted in descending order, with zeros corresponding to the degenerate directions.
    \end{itemize}
    The pseudo-inverse is:
    \begin{equation}
        \Sigma^+ = U \Lambda^+ U^\top,
    \end{equation}
    where $\Lambda^+ = \diag(1/\lambda_1, \dots, 1/\lambda_r, 0, \dots, 0)$.

    Transform the coordinates to the eigen-basis by defining $y = U^\top (x - \mu)$. The Mahalanobis distance simplifies to:

    \begin{equation}
        D_\Sigma(x, \mu) = \sqrt{y^\top \Lambda^+ y} = \sqrt{\sum_{i=1}^r \frac{y_i^2}{\lambda_i}},
    \end{equation}
    since $y_i = 0$ for $i > r$ on the support $\supp{\vz}$. The exponent in the density becomes:   
    \begin{equation}
        -\frac{1}{2} \sum_{i=1}^r \frac{y_i^2}{\lambda_i}.
    \end{equation}

    \item Distance in Different Directions

    The effect of deviations from the mean $\mu $ along the directions of the eigenvectors (principal axes) depends on the corresponding eigenvalues:

    \begin{itemize}
        \item \textbf{Directions with large eigenvalues ($\lambda_i \gg 0 $)}: The term $\frac{y_i^2}{\lambda_i} $ grows slowly as $|y_i| $ increases. Deviations along these high-variance directions contribute less to reducing the density, effectively scaling the distance by $\sqrt{\lambda_i} $. This allows larger Euclidean deviations in these directions.
        \item \textbf{Directions with small positive eigenvalues ($0 < \lambda_i \ll 1 $)}: The term $\frac{y_i^2}{\lambda_i} $ grows rapidly even for small $|y_i| $. Deviations are heavily penalized, scaling the distance by $1 / \sqrt{\lambda_i} $, making small deviations appear ``far'' in the Mahalanobis sense.
        \item \textbf{Directions with zero eigenvalues ($\lambda_i = 0 $)}: These correspond to the null space of $\Sigma$. Here, $y_i $ must be exactly zero for the density to be non-zero, enforced by the Dirac delta measure. Any non-zero deviation results in zero density, equivalent to an infinite Mahalanobis distance, indicating no variability in these directions.
    \end{itemize}
\end{itemize}

\paragraph{Integration on linear subspaces}
The following lemma is useful for change of variable with linear mapping \citep{federer1996geometric}.   
\begin{lemma}[Integration on linear subspaces]\label{lemma:supp_integration_linear_subspace}
    Let $\Normal{y; \mu, \Sigma}$ be the PDF of a Gaussian distribution in $\R^N$, with mean $\mu \in \R^N$ and covariance matrix $\Sigma \in \R^{N \times N}$ of rank $r_0 \leq N$. 
    For any linear application $B: \R^N \to \R^P$, and any measurable $f$, we have:
        \begin{equation}
            \int_{\R^N} f(B y) \Normal{y; \mu, \Sigma} d \Haus^{r_0}(y) = \int_{\R^P} f(v) \Normal{ v; B \mu, B \Sigma B^\top} d \Haus^{r}(v),   
        \end{equation}
        where $\Normal{ v; B \mu, B \Sigma B^\top}$ is the PDF of a (possibly degenerate) Gaussian distribution with respect to the $r-$dimensional Hausdorff measure $\Haus^{r}$, with $r$ being the rank of $B \Sigma B^\top$.
        This PDF is supported on the $r-$dimensional subspace $B \mu + \Im{B \Sigma B^\top} \subset \Im{B} \subset \R^P$. 
\end{lemma}
\begin{proof}
    The proof of this result is relatively straightforward by using the density of a degenerate Gaussian distribution \cref{def:supp_degenerate_gaussian}. 
    Let $\vy \sim \Normal{\mu, \Sigma}$ and $\vz = B \vy$, then $\vz \sim \Normal{B \mu, B \Sigma B^\top}$. We have
    \begin{align}
        \int_{\R^N} f(B y) \Normal{y; \mu, \Sigma} d \Haus^{r_0}(y) &= \Mean{f(B \vy)} \\&= \Mean{f(\vz)} \\&= \int_{\R^P} f(v) \Normal{ v; B \mu, B \Sigma B^\top} d \Haus^{r}(v).
    \end{align}
\end{proof}
\begin{remark}
    When $\Sigma$ is full rank, the Hausdorff measure $\Haus^{r_0}$ coincides with the Lebesgue measure on $\R^N$. 
\end{remark}

\subsection{Minimum Mean Square Error (MMSE) estimator}
The MMSE estimator is the optimal estimator in the following sense
\begin{definition}[MMSE estimator]\label{def:supp_mmse}
    Given two random vectors $\vx \in \X, \vy \in \Y$ and a linear operator $B: \Y \to \X$.
    The MMSE estimator of $\vx$ given $B \vy$ is the best approximation \emph{random variable} $\phi^\star(B \vy)$ to $\vx$, in the least-square sense: 
    \begin{equation}
        \xmmse = \phi^\star \circ B \quad \text{where} \quad \phi^{\star} = \argmin_{\phi : \X \to \X} \Mean{ \norm{\phi(B\vy) - \vx}^2}
    \end{equation}
\end{definition}
It is well-known that the MMSE estimator coincides with the \emph{conditional expectation}. That is, for any $y$ such that $p(y) > 0$, we have: 
\begin{equation}
    \xmmse(y) = \Mean{\vx \vert B \vy = B y} = \int x \cdot p(x \vert B y) dx 
\end{equation}
Composing $\xmmse$ with the random variable $\vy$, we get
$$\xmmse(\vy) = \Mean{\vx \vert B \vy}.$$
In particular, if $B\!=\!\Id$ (in this case $M = N$), the MMSE estimator reduces to the classical \emph{posterior mean}: $\xmmse(y) = \Mean{\vx \vert \vy = y}$. 
Note that both $\Mean{\vx \vert \vy = y}$ and $\Mean{\vx \vert \vy}$ are often called condition expectation, but these are different objects. 
In particular, $\Mean{\vx \vert \vy = \cdot} = \xmmse(\cdot)$ is a function $\Y \to \X$ while $\Mean{\vx \vert \vy}$ is a random variable assuming values in $\X$. 
However, finding the MMSE estimator amounts to finding the optimal function $\phi^\star$.

\subsection{Constrained Minimum Mean Square Error (MMSE) estimator}
The classical MMSE estimator is defined as the best approximation function over the space of measurable function from $\Y$ to $\X$. 
When adding functional constraints to the estimator, we would like to find the best approximation function over a subspace $\M$. 
\begin{equation}
    \min_{\phi \in \M} \Mean{ \norm{\phi(B \vy) - \vx}^2}
\end{equation}
For example, the subspace $\M$ could be the set of measurable functions from $\X \to \X$ and translation equivariant. 
\subsection{Optimality condition}
We first state a simple first-order sufficient and necessary optimality condition for solving the Constrained MMSE. 
\begin{proposition}[Optimality condition]\label{prop:supp_optimality_condition}
    Let $\vx \in \X, \vy \in \Y$ be two random variables and $\M$ be a vector space of measurable functions from $\X$ to $\X$, $B$ be a linear operator from $\Y$ to $\X$. 
    Then $\phi^\star$ is a minimizer of 
    \begin{equation*}
        \min_{\phi \in \M} \Mean{ \norm{\phi(B \vy) - \vx}^2}
    \end{equation*} 
    if and only if
    \begin{equation}
        \Mean{\inner{\varphi(B \vy), \phi^\star(B \vy) - \vx}} = 0 \qquad \text{ for all } \varphi \in \M.
    \end{equation}
    
\end{proposition}
\begin{proof}
    Let $J(\phi) = \Mean{ \norm{\phi(B \vy) - \vx}^2}$. For all $t \in \R$ and for all $\varphi \in \M$, we have
    \begin{align*}
        J(\phi^\star + t \varphi) &= \Mean{\norm{(\phi^\star + t \varphi) (B \vy) - \vx}^2} \\
        &= J(\phi^\star) + 2t \underbrace{\Mean{\inner{\varphi(B \vy), \phi^\star(B \vy) - \vx}} }_{a}  + t^2  \underbrace{ \Mean{\norm{\varphi(B \vy)}^2}}_{b}\\
        &= J(\phi^\star) + 2a t + b t^2 
    \end{align*}
    Therefore, $\phi^\star$ is a minimizer of $J$ on $\M$ if and only if $J(\phi^\star + t \varphi) \geq J(\phi^\star)$ for all $t \in \R$ and all $\varphi \in \M$.
    This is equivalent to the condition that $2a t + b t^2  \geq 0$, for all $t \in \R$ and all $\varphi \in \M$. 
    Since this difference term is a quadratic function in $t$ and $b \geq 0$, it is non-negative for all $t \in \R$ if and only if $a = 0$.
    Therefore, we have the sufficient and necessary condition that $\Mean{\inner{\varphi(B \vy), \phi^\star(B \vy) - \vx}} = 0$ for all $\varphi \in \M$.
\end{proof}

The optimality condition \cref{prop:supp_optimality_condition} states that the residual $\phi^\star(B \vy) - \vx$ is orthogonal, in the $L^2$ sense, to every perturbation in the feasible set $\M$. Equivalently, $\phi^\star(B \vy)$ is the orthogonal projection of $\vx$ onto $\M$ in the $L^2$ sense. When $\M$ is the space of all square-integrable functions of $\vy$ (\ie no constraints) and $B=\Id$, this projection yields the classical MMSE estimator (posterior mean), $\Mean{\vx \vert \vy}$. In the constrained case, $\phi^\star(B \vy)$ can also be viewed as the orthogonal projection of the conditional expectation $\Mean{\vx \vert \vy}$ onto the subspace $\{\phi(B \y) : \phi \in \M \}$.
\begin{proposition}\label{prop:supp_projection_mmse}[\cref{prop:projection_mmse} in the main paper]
    Given a closed set $\M$. 
    The $\M$-constrained MMSE estimator in~\cref{def:constrained_mmse} is the orthogonal projection (in $L^2$ sense) of the posterior mean $\Mean{\vx \vert \vy}$ onto the subspace of $\vy$-measurable random vectors of form $\mathcal{X} = \{ \phi(B\vy) : \phi \in \M \}$.
    That is,
    $$
        \hat{x}_{\M}(\vy) = \Pi_{\mathcal{X}} \, \Mean{\vx \vert \vy}.
    $$
\end{proposition}
\begin{proof}%[Proof of~\cref{prop:projection_mmse}]
The classes $\Mtrans$ and $\Mtransloc$ are linear subspaces of the space of measurable functions from $\R^N$ to $\R^N$: they are closed under addition and scalar multiplication, hence the projection is well-defined.
The MMSE estimator in~\cref{prop:expression_MMSE} is the posterior mean $\Mean{\x \vert \y}$, which is the orthogonal projection of $\x$ onto the space of $\y$-measurable random vectors. 
Similarly, the $\M-$constrained MMSE estimator is the projection of $\x$ on to the subspace $\{\phi(B \y) : \phi \in \M \}$. 
Using the Pythagorean decomposition, for any $\phi \in \M$, we have
\begin{equation}\label{eq:supp_pythagorean_decomposition}
    \Mean{\norm{\phi(B \y) - \x}^2} = \Mean{\norm{\phi(B \y) - \Mean{\x \vert \y}}^2} + \Mean{\norm{\Mean{\x \vert \y} - \x}^2}
\end{equation}  
Therefore
\begin{equation}
    \argmin_{\phi \in \M} \, \Mean{\norm{\phi(B \y) - \x}^2} = \argmin_{\phi \in \M} \, \Mean{\norm{\phi(B \y) - \Mean{\x \vert \y}}^2}
\end{equation}
and the $\M-$constrained MMSE in~\cref{def:constrained_mmse} is the projection of the posterior mean $\Mean{\x \vert \y}$ onto the subspace $\{\phi(B \y) : \phi \in \M \}$.    

Moreover, the decomposition in~\cref{eq:supp_pythagorean_decomposition} has interesting interpretation: it isolates the sources of error in the estimation. 
The first term $\Mean{\norm{\phi(B \y) - \Mean{\x \vert \y}}^2}$ is the approximation error due to the restriction to the subspace $\M$, while the second term $\Mean{\norm{\Mean{\x \vert \y} - \x}^2}$ is the irreducible Bayes error.
\end{proof}

\subsection{Structural constraint: equivariant functions}
\label{sub:definition_equiv}
The below definitions are taken from \citep{Celledoni_2021_equivariant_neural_networks}. 
\begin{definition}[Group] A \emph{group}, to be denoted $\G$, is a set equipped with an associative operator $\cdot : \G \times \G \to \G$, which satisfies the following conditions:
    \begin{enumerate}
        \item If $g_1, g_2 \in \G$ then $g_2 \cdot g_1 \in \G$
        \item If $g_1, g_2, g_3 \in \G$ then $(g_1 \cdot g_2) \cdot g_3 = g_1 \cdot (g_2 \cdot g_3)$
        \item There exists $\id \in \G$ such that $e \cdot g = g \cdot \id = g$ for all $g \in \G$.
        \item If $g \in \G$ there exists $g^{-1} \in \G$ such that $g^{-1} \cdot g = g \cdot g^{-1} = \id$.
    \end{enumerate}    
\end{definition}
\begin{definition}[Group action]
    Given a group $\G$ and a set $\X \subset \R^N$, we say that $\G$ acts on $\X$ if there exists a function $T: \G \times \X \to \X$ (we denote by $T_g(x)$ for $g \in \G$ and $x \in \X$) that satisfies:
    \begin{equation*}
        T_{g_1} \circ T_{g_2} = T_{g_1 \cdot g_2} \qquad \text{and} \qquad T_{\id} = \mathrm{id}
    \end{equation*} 
\end{definition}
Given a general group $\G$. A function $\phi: \X \to \X$ and group action $T$ of $\G$ on $\X$. A function $\phi$ is called $\G$-\emph{equivariant} if it satisfies
\begin{equation*}
    \phi(T_g(y)) = T_g \phi(y) \qquad \text{for all } x \in \X \text{ and for all } g \in \G.
\end{equation*}
\begin{proposition}\label{prop:supp_reynolds_averaging}
    If the group $\G$ is finite, the following properties hold true:
    \begin{enumerate}
        \item \textbf{Invariance}: for any function $\phi: \X \to \R$, the function $\bar{\phi} = \sum_{g} \phi \circ T_g$ is invariant. (And similarly for function defined in $\Y$).  
        \item \textbf{Equivariance}: for any function $\phi: \X \to \X$, the function $\bar{\phi} = \sum_{g} T_g^{-1} \circ \phi \circ T_g$ is equivariant. This is called \textbf{Reynolds averaging}.
    \end{enumerate} 
\end{proposition}

For image data $x \in \R^N$, let $H, W \in \mathbb{N}$ be the dimensions of a discrete grid $\Omega = \Z_H \times \Z_W$ with $H \times W = N$. 
\begin{definition}[Translation equivariant functions]
    Let $\T = \Z_H \times \Z_W$ be the group of 2D cyclic translations. 
    For every group element $g = (g_h, g_v) \in \T$, we define the \emph{translation operator} $T_g: \R^N \to \R^N$ as the permutation matrix that acts on an image $x \in \R^N$ by shifting its indices:
    \[
        (T_g x)[i, j] = x[(i - g_h) \bmod H, \, (j - g_v) \bmod W]
    \]
    for all $(i, j) \in \Omega$.
    A measurable map $\phi: \R^N \to \R^N$ is said to be \emph{translation equivariant} if it commutes with the translation operator for all $g \in \T$:
    \[
        \phi(T_g x) = T_g \phi(x) \qquad \text{for all } x \in \R^N.
    \]
\end{definition}


\subsection{Structural constraints: local and translation equivariant functions}
\label{sub:definition_local_equiv}
We first define precisely the patch extractor. 
Let $n \in \Omega$ be a pixel coordinate on the grid. We define the \emph{patch extractor} $\Pi_n: x \in \X \to \Pi_n x = x[\omega_n] \in \R^P$ which extracts a square patch $x[\omega_n]$ of size $\sqrt{P} \times \sqrt{P}$ centered at $n$.
The extraction uses circular boundary conditions, such that the patch is given by the grid values at indices:
\[
    \omega_n = \{ n + \delta \pmod{(H, W)} \mid \delta \in \Delta \}
\]
where $\Delta$ is the set of offsets defining the square neighborhood centered at zero.

\begin{definition}[Local and translation-equivariant functions]
\label{def:supp_transloc_equiv_fn}
    A measurable map $\phi: \X \to \X$ is said to be \emph{local} if it can be represented as a sliding window operation. 
    Specifically, if there exist a measurable function $f: \R^P \to \R$ such that for all $x \in \X$ and all $n \in \Omega$:
    \[
        \phi(x)[n] = f(\Pi_n x).
    \]
    By construction, any such function $\phi$ is also translation equivariant due to the circular boundary conditions of the patch extractor $\Pi_n$.
    We denote by $\Mtransloc$ the set of all such maps.
\end{definition}

This class captures standard CNNs with finite kernels and weight sharing or patch-based methods, \eg MLPs acting on patches~\citep{glimpse_local}: the output of $\phi$ at pixel $n$, denoted $\phi(x)[n]$ or $\phi_n(x)$, depends only on the local patch (receptive field) $x[\omega_n]$. 
Note that by construction, functions in \cref{def:supp_transloc_equiv_fn} are translation equivariant.


% ############################################################
% ################          THEORY         ###################
% ############################################################

\section{Analytical solution for inverse problems}
In the following, we will derive the analytical solution for the MMSE estimator under these constraints.

Consider a random variable $\vx \sim \pdata, \vx \in \X$ and the forward (measurement) model
\begin{equation}
    \vy = A \vx + \ve \qquad \text{ where } \qquad \ve \sim \Normal{0, \sigma^2 \Id}.
\end{equation}
We would like to find the MMSE estimator of $\vx$ given $\vy$, with or without constraints. 
In this section, we will derive the analytical solution for the MMSE estimator under various constraints, when the true underlying distribution of $\vx$ is replaced by the empirical distribution $\ptrain$. 
Under Gaussian noise, the likelihood of the measurement $\vy$ given $\vx$ is given by
\begin{equation}
    p(y \vert x) = \Normal{y; A x, \sigma^2 \Id} \propto \exp\left( -\frac{\norm{y - A x}^2}{2 \sigma^2} \right).
\end{equation}
Therefore, for any function $\phi^{\star}$ and $\varphi$, we have
\begin{align}
    \Mean{\inner{\varphi(B \vy), \phi^\star(B \vy) - \vx}} &= \E_{\x \sim \ptrain} \E_{\vy \vert \vx} \left[ \inner{\varphi(B \vy), \phi^\star(B \vy) - \vx} \right] \notag \\
    &= \frac{1}{|\D|} \sum_{x \in \D} \int_{\Y} \inner{\varphi(B y), \phi^\star(B y) - x} p(y \vert x) dy \notag \\
    &= \frac{1}{|\D|} \sum_{x \in \D} \int_{\Y} \inner{\varphi(B y), \phi^\star(B y) - x} \Normal{y; A x, \sigma^2 \Id} dy \label{eq:supp_linear_term}
\end{align}
This simplified expression (\ref{eq:supp_linear_term}) is useful for deriving the analytical solution of the MMSE estimator under various constraints.

\subsection{Unconstrained MMSE estimator}
We start with the unconstrained MMSE estimator, which is the optimal estimator in the least-square sense. Then, we will derive the equivariant MMSE estimator, which is the optimal estimator under the constraint of equivariance to translation. Then, we will derive the local MMSE estimator, which is the optimal estimator under the constraint of locality. Finally, we will also show that combining both constraints leads to a local and equivariant MMSE estimator.
\begin{proposition}[Unconstrained MMSE estimator]\label{prop:supp_unconstrained_mmse}
    When the data distribution is replaced by the empirical distribution $\ptrain$, the unconstrained MMSE estimator of $\vx$ given $\vy$ is given by
    \begin{equation}
        \xmmse(y) = \phi^\star(B y) \quad \text{where} \quad \phi^\star(v) = \frac{\sum_{x \in \D} x \cdot \Normal{v; B A x, \sigma^2 BB^\top}}{\sum_{x \in \D} \Normal{v; B A x, \sigma^2 BB^\top}} \quad \text{for any } v \in \Im{B}. 
    \end{equation}
    The estimator is well-defined for all $y \in \Y$.     
\end{proposition}
\begin{proof}
    We will verify that $\phi^*$ satisfies the optimality condition from \cref{prop:supp_optimality_condition}.
    Using \cref{eq:supp_linear_term}, for any $\varphi$, we have:
    \begin{align}
        \Mean{\inner{\varphi(B \y), \phi^*(B \y) - \x}} &=\frac{1}{|\D|}\sum_{x\in\D}\int_{\R^M} \langle \varphi(B y), \phi^*(B y) - x\rangle \Normal{y,Ax,\sigma^2 \Id_M} dy \notag\\
        &=\frac{1}{|\D|}\sum_{x\in\D}\int_{\R^N} \langle \varphi(v), \phi^*(v) - x \rangle \Normal{v,BAx,\sigma^2 BB^\top} d \Haus^r(v)\label{eq:proof_unconstrained_mmse_change_of_variable}\\
        &= \frac{1}{|\D|}\int_{\R^N} \left\langle \varphi(v), \phi^*(v) \sum_{x\in\D}\Normal{v,BAx,\sigma^2 BB^\top}-x \sum_{x\in\D}\Normal{v,BAx,\sigma^2 BB^\top}\right\rangle d \Haus^r(v) \notag\\
        &=0 \notag
    \end{align} 
    where \cref{eq:proof_unconstrained_mmse_change_of_variable} comes from the change of variable $v = B y$ and \cref{lemma:supp_integration_linear_subspace}.
    The last equality holds by definition of $\phi^*$. 
\end{proof}

\subsection{Equivariant MMSE estimator}\label{sec:supp_equivariant_mmse}

\begin{proposition}[Translation equivariant MMSE estimator]\label{prop:supp_equivariant_mmse}
    The translation equivariant MMSE estimator is as follows, for any $y \in \Y$:
        \begin{equation}
            \xtrans(y) = \phi^\star(B y) \qquad \text{where} \qquad \phi^\star(v) 
            =   \frac{\sum_{x \in \D, g \in \T} T_g x \cdot \Normal{T_g^{-1} v; B A x, \sigma^2 B B^\top}}{\sum_{x \in \D, g \in \T}  \Normal{T_g^{-1} v; B A x, \sigma^2 B B^\top}}
        \end{equation}
        for any $v \in \bigcup_{g} T_g \Im{B} \subset \X$.
\end{proposition}
\begin{proof}
    We will verify that $\phi^\star$ is admissible and satisfies the optimality condition \cref{prop:supp_optimality_condition}.

    \textbf{Admissibility.} 
    Let $q(v, x) = \Normal{v; B A x, \sigma^2 B B^\top}$ denote the PDF of a Gaussian distribution with mean $B A x$ and covariance $\sigma^2 B B^\top$. 
    The optimal estimator can be written as:
    \begin{equation}
        \phi^\star(v) = \frac{\sum_{x \in \D, g \in \T} T_g x q(T_g^{-1} v, x)}{\sum_{x \in \D, g \in \T}  q(T_g^{-1} v, x)}
    \end{equation}
    The denominator $\sum_{x \in \D, g \in \T}  q(T_g^{-1} v, x) = \sum_{g \in \T} \bar{q}_1(T_g^{-1} v)$ is $\T-$invariant by \cref{prop:supp_reynolds_averaging}, 
    where $\bar{q}_1(v) = \sum_{x \in \D} q(v, x)$.
    
    The nominator $\sum_{x \in \D, g \in \T} T_g x \cdot q(T_g^{-1} v, x) = \sum_{g \in \T} T_g \bar{q}_2(T_g^{-1} v)$ is $\T-$equivariant, where $\bar{q}_2(v) = \sum_{x \in \D} x \cdot q(v, x)$.
    Therefore, $\phi^\star$ is admissible, that is $\phi^\star \in \Mtrans$. 

    \textbf{Optimality condition.}
    By using \cref{eq:supp_linear_term}, for any $\varphi \in \Mtrans$, we have:
    \allowdisplaybreaks
    \begin{align}
        &\Mean{\inner{\varphi(B \y), \phi^\star(B  \y) - \vx}}  \notag \\
        &=  \frac{1}{|\D|}  \sum_{x \in \D} \int_{\Y} \inner{\varphi(B  y), \phi^\star(B  y) - x } \Normal{y; A x, \sigma^2 \Id_N} dy \notag \\
        &= \frac{1}{|\D|}  \sum_{x \in \D} \int_{\X} \inner{\varphi(v), \phi^\star(v) - x } \Normal{v; B A x, \sigma^2 B B^\top} d \Haus^r(v) \label{supp_proof:equiv_est_change_variable_operator} \\
        &= \frac{1}{|\D| |\T|} \sum_{g \in \T, x \in \D} \int_{\X} \inner{ T_g^{-1} \varphi(T_g v), T_g^{-1} \phi^\star( T_g v)
         - x } \Normal{v; B A x, \sigma^2 B B^\top} d \Haus^r(v)  \label{supp_proof:equiv_est_translation} \\
        &= \frac{1}{|\D| |\T|} \sum_{g \in \T, x\in \D} \int_{\X} \inner{\varphi(T_g v), \phi^\star(T_g v) - T_g x }  \Normal{v; B A x, \sigma^2 B B^\top} d\Haus^r(v) \notag \\
        &= \frac{1}{|\D| |\T|} \sum_{g \in \T, x\in \D} \int_{\X} \inner{\varphi(v), \phi^\star(v) - T_g x } \Normal{T_g^{-1} v; B A x, \sigma^2 B B^\top} d\Haus^r(v) \label{supp_proof:equiv_change_variable_translation} \\
        &= \frac{1}{|\D| |\T|} \int_{\X} \inner{\varphi(v), \phi^\star(v) \sum_{g \in \T, x\in \D} q(T_g^{-1} v, n, x) - \sum_{g \in \T, x\in \D} T_g x \cdot q(T_g^{-1} v, n, x)  } d\Haus^r(v) \notag \\
        &= 0 \notag
    \end{align}
    where $q(T_g^{-1} v, n, x) \eqdef  \Normal{T_g^{-1} v; B A x, \sigma^2 B B^\top}$ and
    \begin{itemize}
        \item In~\cref{supp_proof:equiv_est_change_variable_operator}, we apply \cref{lemma:supp_integration_linear_subspace} for $B$ in the change of variables. 
        \item In~\cref{supp_proof:equiv_est_translation}, we use the fact that $\varphi = \varphi \circ T_g \circ T_g^{-1} = T_g \circ \varphi \circ T_g^{-1}$ for all $g \in \T$ and $\varphi \in \Mtrans$. Moreover, $T_{g}^{-1} = T_g^{\top}$ for all $g \in \T$. 
        \item In~\cref{supp_proof:equiv_change_variable_translation}, we use the change of variable $T_g v \to v$ and the fact that $T_g$ is an isometry.
    \end{itemize} 
\end{proof}

We next state and prove that the augmented MMSE estimator is reconstruction equivariant.
    \begin{lemma}[Reconstruction equivariance of $\xmmseaug$]\label{lemma:supp_reconstruction_equivariance}
        Let $A: \R^N \to \R^M$ be a circular convolution operator, i.e., $A T_g = T_g A$ for all $g \in \T$.
        Then, the data-augmented MMSE estimator $\xmmseaug$ defined in\cref{def:data_augmented_mmse} satisfies the reconstruction equivariance property in~\cref{eq:reconstruction_equivariance}:
        \begin{equation*}
            \xmmseaug(A T_g \bar{x} + e) = T_g \xmmseaug(A \bar{x} + T_g^{-1} e)
        \end{equation*}
        for all $\bar{x} \in \R^N$, all $e \in \R^M$, and all $g \in \T$.
    \end{lemma}

    \begin{proof}
    Let $y = A T_g \bar{x} + e$ be the input observation.
    Recall the definition of the data-augmented MMSE estimator in~\cref{def:data_augmented_mmse}:
    \begin{equation*}
        \xmmseaug(y) = \frac{\sum_{x \in \T(\D)} x \cdot \exp\left( -\frac{1}{2\sigma^2} \norm{y - Ax}^2 \right)}{\sum_{x \in \T(\D)} \exp\left( -\frac{1}{2\sigma^2} \norm{y - Ax}^2 \right)}.
    \end{equation*}
    Substituting the input $y = A T_g \bar{x} + e$ into the estimator:
    \begin{equation*}
        \xmmseaug(y) = \frac{\sum_{x \in \T(\D)} x \cdot \exp\left( -\frac{1}{2\sigma^2} \norm{A T_g \bar{x} + e - Ax}^2 \right)}{\sum_{x \in \T(\D)} \exp\left( -\frac{1}{2\sigma^2} \norm{A T_g \bar{x} + e - Ax}^2 \right)}.
    \end{equation*}
    Since $\T(\D)$ is invariant under the group action, we can perform a change of variable $z = T_g^{-1} x$. As $x$ traverses $\T(\D)$, $z$ also traverses $\T(\D)$. Note that $x = T_g z$.
    Substituting $x$ with $T_g z$ in the numerator and denominator:
    \begin{equation*}
        \xmmseaug(y) = \frac{\sum_{z \in \T(\D)} T_g z \cdot \exp\left( -\frac{1}{2\sigma^2} \norm{A T_g \bar{x} + e - A T_g z}^2 \right)}{\sum_{z \in \T(\D)} \exp\left( -\frac{1}{2\sigma^2} \norm{A T_g \bar{x} + e - A T_g z}^2 \right)}.
    \end{equation*}
    Using the assumption that $A$ is a circular convolution ($A T_g = T_g A$) and $T_g$ is linear, the term inside the norm becomes:
    \begin{align*}
        A T_g \bar{x} + e - A T_g z &= T_g A \bar{x} + T_g (T_g^{-1} e) - T_g A z \\
        &= T_g \left( A \bar{x} + T_g^{-1} e - A z \right).
    \end{align*}
    Since $T_g$ is unitary, it preserves the norm, i.e., $\norm{T_g u} = \norm{u}$. Therefore:
    \begin{equation*}
        \norm{A T_g \bar{x} + e - A T_g z}^2 = \norm{A \bar{x} + T_g^{-1} e - A z}^2.
    \end{equation*}
    Substituting this back into the estimator expression and factoring the linear operator $T_g$ out of the sum in the numerator:
    \begin{align*}
        \xmmseaug(y) &= \frac{T_g \sum_{z \in \T(\D)} z \cdot \exp\left( -\frac{1}{2\sigma^2} \norm{(A \bar{x} + T_g^{-1} e) - A z}^2 \right)}{\sum_{z \in \T(\D)} \exp\left( -\frac{1}{2\sigma^2} \norm{(A \bar{x} + T_g^{-1} e) - A z}^2 \right)} \\
        &= T_g \left( \frac{\sum_{z \in \T(\D)} z \cdot \Normal{A \bar{x} + T_g^{-1} e; A z, \sigma^2 \Id}}{\sum_{z' \in \T(\D)} \Normal{A \bar{x} + T_g^{-1} e; A z', \sigma^2 \Id}} \right).
    \end{align*}
    The term in the parentheses is exactly the estimator evaluated at the input $A \bar{x} + T_g^{-1} e$. Thus:
    \begin{equation*}
        \xmmseaug(A T_g \bar{x} + e) = T_g \xmmseaug(A \bar{x} + T_g^{-1} e).
    \end{equation*}
    \end{proof}

Now we state and prove the properties of the E-MMSE estimator presented in~\cref{cor:equivariant_mmse_properties}.

\begin{proof}[Proof of~\cref{cor:equivariant_mmse_properties}]\label{proof:supp_equivariant_properties}
    The first point is a direct consequence of~\cref{thm:equivariant_mmse}.  
    We prove the second and third point as follows.
    We first express the weights of both estimators.
    The two estimators admit the same form:
    \begin{equation*}
        \xtrans(y) = \sum_{x \in \D, g \in \T} T_g x \cdot w_g(x \vert y) \qquad \text{and} \qquad \xmmseaug(y) = \sum_{x \in \D, g \in \T} T_g x \cdot w_g^{\text{aug}}(x \vert y).
    \end{equation*}
    The weights of the E-MMSE estimator $\xtrans$ corresponding to the component $(x, g)$ are given by:
    \begin{align*}
        w_g(x \vert y) 
        &= \frac{\Normal{T_g^{-1} B y; B A x, \sigma^2 B B^{\top}}}{\sum_{x' \in \D, g' \in \T} \Normal{T_{g'}^{-1} B y; B A x', \sigma^2 B B^{\top}}}
        \\&= \frac{\exp \left( -\norm{B^{-1} (T_g^{-1} B y - B A x)}^2 / (2 \sigma^2) \right)}{\sum_{x' \in \D, g' \in \T} \exp \left( - \norm{B^{-1} (T_{g'}^{-1} B y - B A x')}^2 / (2 \sigma^2) \right)}
        \\&= \frac{\exp \left( -\norm{B^{-1} T_g^{-1} B y - A x}^2 / (2 \sigma^2) \right)}{\sum_{x' \in \D, g' \in \T} \exp \left( -\norm{B^{-1} T_{g'}^{-1} B y - A x'}^2 / (2 \sigma^2) \right)}
    \end{align*}
    where we used the invertibility of $B$ to write $B^+ = B^{-1}$ and simplified the Mahalanobis distance.
    The weights of the data-augmented estimator $\xmmseaug$ are:
    \begin{align*}
        w_g^{\text{aug}}(x \vert y) 
        &= \frac{\Normal{B y; B A T_g x, \sigma^2 B B^\top}}{\sum_{x' \in \D, g' \in \T} \Normal{B y; B A T_{g'} x', \sigma^2 B B^\top}}
        \\&= \frac{\exp \left( -\norm{B^{-1}(B y - B A T_g x)}^2 / (2 \sigma^2) \right)}{\sum_{x' \in \D, g' \in \T} \exp \left( -\norm{B^{-1}(B y - B A T_{g'} x')}^2 / (2 \sigma^2) \right)}
        \\&= \frac{\exp \left( -\norm{y - A T_g x}^2 / (2 \sigma^2) \right)}{\sum_{x' \in \D, g' \in \T} \exp \left( -\norm{y - A T_{g'} x'}^2 / (2 \sigma^2) \right)}.
    \end{align*}
    
    \paragraph{Sufficiency ($\Leftarrow$)}
    Assume $A$ and $B$ are circular convolutions (they commute with $T_g$) and $B$ is invertible.

    Commutativity implies $B^{-1} T_g^{-1} = T_g^{-1} B^{-1}$. We can rearrange the term in the exponent of the above weights as follows:
    \begin{equation*}
        \norm{B^{-1} T_g^{-1} B y - A x}^2
        = \norm{B^{-1} B T_g^{-1} y - B^{-1} B A x}^2
        = \norm{T_g^{-1} y - A x}^2.
    \end{equation*}
    Since $T_g$ is unitary and $A$ commutes with $T_g$, we have:
    \begin{equation*}
        \norm{T_g^{-1} y - A x}^2 = \norm{T_g(T_g^{-1} y - A x)}^2 = \norm{y - T_g A x}^2 = \norm{y - A T_g x}^2.
    \end{equation*}
    Thus $\xtrans = \xmmseaug$. 
    Furthermore, since the weights are independent of $B$, the physics-agnostic and physics-aware solvers are identical. 
    The reconstruction equivariance follows immediately from~\cref{lemma:supp_reconstruction_equivariance}.

\paragraph{Necessity ($\Rightarrow$)}
Assume that  $w_g^{\text{aug}}(x \vert y) =  w_g(x \vert y)$ for all $y \in \R^M$ and all $x\in \R^N$ and that $A$ and $B$ are invertible.
This implies that:
\begin{equation}\label{eq:nec_energy_equality}
    \norm{B^{-1} T_g^{-1} B y - A x}^2 = \norm{y - A T_g x}^2 + c(y)
\end{equation}
for some function $c(y)$ independent of $x$ and $g$. We expand the squared norms and the terms depending solely on $y$ can be absorbed into $c(y)$.
\begin{align*}
    \text{LHS}
    &= \underbrace{\norm{B^{-1} T_g^{-1} B y}^2}_{\text{depends only on } y} - 2 \langle B^{-1} T_g^{-1} B y, A x \rangle + \norm{A x}^2, \\
    \text{RHS} &= \underbrace{\norm{y}^2}_{\text{depends only on } y} - 2 \langle y, A T_g x \rangle + \norm{A T_g x}^2 + c(y).
\end{align*}
For the equality in \cref{eq:nec_energy_equality} to hold for all $x\in \R^N$ and $y\in \R^M$, the cross-terms (the bilinear forms coupling $y$ and $x$) must be identical.
Moreover, since the estimator $\xmmseaug$ is \emph{independent} of $B$, the~\cref{eq:nec_energy_equality} must hold for all invertible $B$.
In particular, taking $B = \Id$, we deduce that
\begin{equation*}
    \inner{T_g^{-1}y, Ax} = \inner{y, A T_g x} \quad \text{for all } x \in \R^N, g \in \T, y \in \R^M.
\end{equation*}
This equality yields $T_g A = A T_g$, or that $A$ commutes with $T_g$.
Plugging this result into the cross-term equality for a general invertible $B$, we get:
\begin{equation*}
    \inner{B^{-1} T_g^{-1} B y, A x} = \inner{y, A T_g x} = \inner{T_g^{-1} y, A x} \quad \text{for all } x \in \R^N, g \in \T, y \in \R^M.
\end{equation*}
This implies that $A^\top B^{-1} T_g^{-1} B = A^\top T_g^{-1}$ for all $g$. 
Since $A$ is invertible, this implies that $B^{-1} T_g^{-1} B = T_g^{-1}$ for all $g\in \T$.  
Multiplying by $B$ on each side of the equality implies that $B$ commutes with $T_g$, concluding the proof.
\end{proof}


\subsection{Local and Translation Equivariant MMSE estimator}\label{sec:supp_local_trans_mmse}
\begin{proposition}[Local and translation equivariant MMSE estimator]
    \label{prop:supp_local_trans_estimator}
        Suppose that the $N$ matrices $Q_{n} = \Pi_n B \in \R^{P \times M}$ have \textbf{constant rank} $r>0$.
        The local and equivariant MMSE is defined for any $y \in \Y$ by:  
        \begin{equation}
            \xtransloc(y) = \phi^\star(B y),
        \end{equation}
        where
        \begin{equation}
            \phi^\star_n = f_{\phi^\star} \circ \Pi_n \qquad \text{with} \qquad
            f_{\phi^\star}(v) = \frac{\sum_{x \in \D} \sum_{n = 1}^{N} x_n \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}{\sum_{x \in \D} \sum_{n = 1}^{N} \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}.
        \end{equation}
		% {\color{blue}(there is no $v$ in the formula for $f$, I guess this is $z$. Also in the proof, this could be harmonized as $z$ turn to $v$ sometimes.)}
\end{proposition}
\begin{proof}
    We will verify that $\phi^\star$ is admissible and satisfies the optimality condition \cref{prop:supp_optimality_condition}.

    \textbf{Admissibility.}
    Firstly, we show that $\phi^\star \in \Mtransloc$. By construction, we have $\phi^\star = (\phi^\star_1, \cdots \phi^\star_N) = (f_{\phi^\star} \circ \Pi_1, \cdots f_{\phi^\star} \circ \Pi_N)$, therefore $\phi^{\star}$ is local. 
    For any $g \in \left\{ 1, \cdots N \right\}$, the group transformation (translation) $T_g$ is defined as 
    $T_g : (x_1, \cdots, x_N) \in \X \mapsto (x_{1 - g}, \cdots, x_{N - g}) \in \X$, where $x_{n - g} = x_{n - g \equiv N}$ (circular boundary). For any $g$, we have 
    \begin{align*}
        \phi^\star \circ T_g &=  (\phi^\star_1 \circ T_g , \cdots \phi^\star_N \circ T_g) \\
        &= (f_{\phi^\star} \circ \Pi_1 \circ T_g, \cdots f_{\phi^\star} \circ \Pi_N \circ T_g)\\
        &= (f_{\phi^\star} \circ \Pi_{1 - g}, \cdots f_{\phi^\star} \circ \Pi_{N - g})  \qquad \text{ where } \Pi_{n - g} = \Pi_{n - g \equiv N} \text{ (circular boundary)}\\
        &= T_g \circ \phi^\star 
    \end{align*}
    Therefore, $\phi^{\star}$ is translation equivariant. We deduce that $\phi^\star \in \Mtransloc$. 

    \textbf{Optimality condition.} We will verify that this estimator satisfies the optimality condition \cref{prop:supp_optimality_condition}.
    By using \cref{eq:supp_linear_term}, for any $\varphi \in \Mtransloc$, we have:
    \allowdisplaybreaks
    \begin{align*}
        &\Mean{\inner{\varphi(B \y), \phi^\star(B  \y) - \x}}  \\
        &= \frac{1}{|\D|}  \sum_{x \in \D} \int_{\Y} \inner{\varphi(B  y), \phi^\star(B  y) - x } \Normal{y; A x, \sigma^2 \Id_N} dy \notag \\
        &= \frac{1}{|\D|}  \sum_{x \in \D} \sum_{n = 1}^{N} \int_{\Y} f_{\varphi} (\Pi_n B  y) \left( f_{\phi^\star} (\Pi_n B y) - x_n  \right) \Normal{y; A x, \sigma^2 \Id_N} dy \notag \\
        &= \frac{1}{|\D|}  \sum_{x \in \D} \sum_{n = 1}^{N} \int_{\R^P} f_{\varphi} (v) \left( f_{\phi^\star} (v) - x_n  \right) \underbrace{\Normal{v; Q_{n} A x, \sigma^2 Q_{n} Q_{n}^\top}}_{q(v, n, x)} d \Haus^r(v) \notag \\
        &= \frac{1}{|\D|} \int_{\R^P} f_{\varphi} (v) \left( f_{\phi^\star} (v) \sum_{x \in \D} \sum_{n = 1}^{N}  q(v, n, x) - \sum_{x \in \D} \sum_{n = 1}^{N} x_n q(v, n, x) \right) d \Haus^r(v) \notag \\
        &= 0
    \end{align*}
\end{proof}

The constant rank assumption in~\cref{prop:supp_local_trans_estimator} can be relaxed by stratifying the image space according to the rank of the matrices $Q_n$ as follows.
\begin{proposition}[Local and translation equivariant MMSE estimator -- rank stratification]
    \label{prop:supp_local_trans_estimator_general}
    Let $Q_n = \Pi_n B \in \R^{P \times M}$. For each rank $r \in \{0,1,\dots,P\}$, define the index set and the union of subspaces:
    $$
        I_r = \{ n \in [1\!:\!N] : \rank{Q_n} = r\}, \quad 
        E_r = \bigcup_{n \in I_r} \Im{Q_n} \subset \R^P.
    $$
    We define the disjoint strata recursively: $\bar{E}_r = E_r \setminus \bigcup_{r' < r} E_{r'}$.
    Then $\bigcup_{r=0}^{P} \bar{E}_r = \bigcup_{n=1}^{N} \Im{Q_n}$ is a disjoint union and, for any $r' < r$, we have $\Haus^r(E_{r'}) = 0$.
    
    Define the weighted density sums for $v \in \R^P$:
    \begin{align*}
        a_r(v) &= \sum_{x \in \D} \sum_{n \in I_r} x_n \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}, \\
        b_r(v) &= \sum_{x \in \D} \sum_{n \in I_r} \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}.
    \end{align*}
    The local and translation equivariant MMSE estimator $\phi^\star$ is given component-wise by $\phi^\star_n = f_{\phi^\star} \circ \Pi_n$, where:
    \begin{equation}
        f_{\phi^\star}(v) = 
        \begin{cases}
             \sum_{r=0}^{P} \frac{a_r(v)}{b_r(v)} \mathbbm{1}_{\bar{E}_r}(v) & \text{if } v \in \bigcup \Im Q_n \text{ and } b_r(v) > 0, \\
             0 & \text{otherwise}.
        \end{cases}
    \end{equation}
\end{proposition}

\begin{proof}
    \textbf{Admissibility (locality and translation equivariance).}
    \begin{itemize}
        \item Locality. By definition, the estimator is defined component-wise by $\phi^\star_n = f_{\phi^\star} \circ \Pi_n$, so it is local.

        \item Translation equivariance on $\Im B$. Let $T_g$ denote the circular translation on $\X$, the selection operators commute with $T_g$ by a simple index check 
        $$ \Pi_n \circ T_g = \Pi_{n-g}, \qquad \forall\,n,g\in \llbracket 1, N \rrbracket.$$
        Then, for any $v\in \Im B$ and any $n$,
        \begin{equation}
            \big[\phi^\star(T_g v)\big]_n
            = f_{\phi^\star}\big(\Pi_n T_g v\big) 
            = f_{\phi^\star}\big(\Pi_{n-g} v\big)
            = \big[\phi^\star(v)\big]_{n-g}
            = \big[T_g \phi^\star(v)\big]_n.
        \end{equation}
        Hence $\phi^\star \circ T_g = T_g \circ \phi^\star$ on $\Im B$, i.e., it is translation equivariant. Combining with locality gives $\phi^\star \in \Mtransloc$.
        
        \item Well-definedness. On each $\bar{E}_r$, $f_{\phi^\star}$ is defined by the ratio $a_r/b_r$. 
        If $b_r(v)=0$ on a negligible set (w.r.t. $\Haus^r$), assign any fixed value (\eg $0$); this does not affect admissibility nor optimality.
    \end{itemize}

    \textbf{Optimality.} For any $\varphi \in \Mtransloc$, we will verify that the optimality condition \cref{prop:supp_optimality_condition} holds.
    By using \cref{eq:supp_linear_term}, we have:
    \begin{align*}
        &\Mean{\inner{\varphi(B \vy), \phi^\star(B \vy) - \vx}} \\
        &= \frac{1}{|\D|}  \sum_{x \in \D} \int_{\Y} \inner{\varphi(B  y), \phi^\star(B  y) - x } \Normal{y; A x, \sigma^2 \Id_N} dy \notag \\
        &= \frac{1}{|\D|} \sum_{x \in \D} \sum_{n=1}^{N} \int_{\Y} f_{\varphi}(\Pi_n B y) \big( f_{\phi^\star}(\Pi_n B y) - x_n \big) \Normal{y; A x, \sigma^2 I} \, dy \\
        &\overset{(\star)}{=} \frac{1}{|\D|} \sum_{x \in \D} \sum_{n=1}^{N} \int_{\R^P} f_{\varphi}(v) \big( f_{\phi^\star}(v) - x_n \big) \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top} \, d\Haus^{r_n}(v),
    \end{align*}
    where $(\star)$ uses \cref{lemma:supp_integration_linear_subspace} and $r_n = \rank{Q_n}$.
    We decompose the summation over $n$ by rank. Note that for $n \in I_r$, the domain is $\Im Q_n$. We observe that $\Im Q_n \setminus \bar{E}_r \subseteq \bigcup_{r' < r} E_{r'}$. Since the union of lower-dimensional subspaces has $\Haus^r$-measure zero, we can restrict the integration domain from $\Im Q_n$ to $\Im Q_n \cap \bar{E}_r$ without changing the value of the integral. 
    Thus, the previous expression becomes: 
    \begin{align}
        \sum_{r=0}^{P} &\int_{\bar{E}_r}
            f_{\varphi}(v) \Big( f_{\phi^\star}(v) \underbrace{\sum_{x \in \D} \sum_{n \in I_r} \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}_{b_r(v)}
            \\& \hspace{4cm}
            - \underbrace{\sum_{x \in \D} \sum_{n \in I_r} x_n \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}_{a_r(v)} \Big) d\Haus^{r}(v).
    \end{align}
    Choosing $f_{\phi^\star}(v)=a_r(v)/b_r(v)$ on $\bar{E}_r$ cancels each integrand, hence the optimality condition \cref{prop:supp_optimality_condition} holds.
\end{proof}

\begin{remark}
    If all $Q_n$ have the same rank $r$, then $\bar{E}_r = \bigcup_{n=1}^{N} \Im{Q_n}$ and the formula reduces to the constant-rank case in \cref{prop:supp_local_trans_estimator}, with a single ratio over $n=1,\dots,N$.
\end{remark}

\begin{remark}[Singular limit and rank stratification]\label{remark:rank_deficient_B}
    Consider a regularization of $B$ as $B^{(\epsilon)} = U \Sigma_\epsilon V^\top$ and $Q_n^{(\epsilon)} = \Pi_n B^{(\epsilon)}$, where $B = U \Sigma V^\top$ is the SVD of $B$ 
    and the diagonal matrix $\Sigma_\epsilon$ is constructed by adding a small positive number $\epsilon$ to the singular values in $\Sigma$.  
    For $\epsilon>0$ (and $M\ge P$), each $Q_n^{(\epsilon)}$ has full row rank $P$. For each $(n,x)$, define the probability measure $\mu_{n,x}^{(\epsilon)}$ on $\R^P$ with density (Radon--NikodÃ½m derivative) with respect to Lebesgue measure $\lambda^P$ given by
    $$
        \frac{d\mu_{n,x}^{(\epsilon)}}{d\lambda^P}(v)
        \;=\; \Normal{v; Q_n^{(\epsilon)} A x, 
        \sigma^2 Q_n^{(\epsilon)} Q_n^{(\epsilon)T}}.
    $$
    As $\epsilon \to 0$, $Q_n^{(\epsilon)} \to Q_n$ and some ranks $r_n = \rank Q_n$ may drop. Then $\mu_{n,x}^{(\epsilon)}$ converges weakly to a probability measure $\mu_{n,x}$ supported on the linear subspace $\Im Q_n$, which is absolutely continuous with respect to the Hausdorff measure $\Haus^{r_n}$ on $\Im Q_n$, with density
    $$
        \frac{d\mu_{n,x}}{d\Haus^{r_n}}(v)
        \;=\; \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top},
        \qquad v \in \Im{Q_n},
    $$
    where, $\Normal{\cdot;\mu,\Sigma}$ denotes the degenerate Gaussian density~\cref{def:supp_degenerate_gaussian} with respect to the appropriate Hausdorff measure on its support (and vanishes off that support).

    For $\epsilon>0$, the estimator reads (constant-rank case, similar to \cref{prop:supp_local_trans_estimator})
    $$
        f_{\phi^\star}^{(\epsilon)}(v)
        \;=\; \frac{\sum_{x \in \D} \sum_{n=1}^{N} x_n \, \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}{\sum_{x \in \D} \sum_{n=1}^{N} \Normal{v; Q_n A x, \sigma^2 Q_n Q_n^\top}}.
    $$
    In the singular limit, for each $r$ and $\Haus^r$-a.e. $v \in \bar{E}_r$ with $b_r(v)>0$,
    $$
        \lim_{\epsilon\to 0} f_{\phi^\star}^{(\epsilon)}(v)
        \;=\; \frac{a_r(v)}{b_r(v)},
    $$
    \ie the $\epsilon$-regularized estimator converges (stratum-wise, $\Haus^r$-a.e.) to the rank-stratified formula
    $$
        f_{\phi^\star}(v) 
        \;=\; \sum_{r=0}^{P} \frac{a_r(v)}{b_r(v)}\, \mathbbm{1}_{\bar{E}_r}(v),
    $$
    interpreted up to $\Haus^r$-null sets on each $\bar{E}_r$. If all $Q_n$ have the same rank $r$, only the stratum $\bar{E}_r$ is nonempty, and the above reduces to the constant-rank expression in ~\cref{prop:supp_local_trans_estimator}.
\end{remark}

\begin{proof}[Proof of i) of~\cref{cor:local_trans_estimator_properties} --- Physics-agnostic LE-MMSE estimator]
    When $B = \Id$, the weights of the LE-MMSE estimator in~\cref{theorem:local_trans_estimator} simplifies to 
    \begin{equation*}
        w_{n', n}(x \vert y) \propto \Normal{\Pi_{n'} y; \Pi_{n} A x, \sigma^2 \Pi_{n} \Pi_{n}^\top} \propto \exp \left( - \frac{1}{2\sigma^2} \norm{y[\omega_{n'}] - (A x)[\omega_{n}]}^2 \right)
    \end{equation*}
    Therefore, the LE-MMSE estimator at pixel $n'$ reads:
    \begin{equation*}
        \xtransloc(y)[n'] = \frac{\sum_{x \in \D} \sum_{n = 1}^{N} x_n \exp \left( - \frac{1}{2\sigma^2} \norm{y[\omega_{n'}] - (A x)[\omega_{n}]}^2 \right)}{\sum_{x \in \D} \sum_{n = 1}^{N} \exp \left( - \frac{1}{2\sigma^2} \norm{y[\omega_{n'}] - (A x)[\omega_{n}]}^2 \right)}.
    \end{equation*}
    For small noise level $\sigma \to 0$, it returns the central pixel value of the patches in the dataset $\D$ whose degraded version $(A x)[\omega_n]$ is closest to the observed patch $y[\omega_{n'}]$. Hence, the LE-MMSE estimator is a patch-work of training patches.
\end{proof}
\begin{proof}[Proof of ii) of~\cref{cor:local_trans_estimator_properties} --- LE-MMSE is not a posterior mean]
    We focus on the simplest case of denoising, that is $\vy = \vx + \ve$.
    Recall that the posterior mean satisfies the so-called Tweedie formula:
    \begin{equation*}
        \Mean{\vx \vert y} = y + \sigma^2 \nabla \log p_{\vy}(y).
    \end{equation*}
    Taking the gradient of both sides w.r.t $y$ yields:
    \begin{equation*}
        \nabla_y \Mean{\vx \vert y} = \Id + \sigma^2 \nabla^2 \log p_{\vy}(y).
    \end{equation*}
    Therefore, the Jacobian of any pure MMSE estimator (posterior mean) must be symmetric since the Hessian of $\log p_{\vy}$ is symmetric. 
    To simplify the notation, we use $f$ instead of $\xtransloc$ in what follows.
    Using the fact that the scaling by $\sigma^{-2}$ does not affect the symmetry (note that $p_{\vy}$ is the convolution of $p_{\vx}$ and a Gaussian so it's $\mathcal{C}^{\infty}$), if $\xtransloc$ were a posterior mean, we must have:
    \begin{equation}\label{eq:supp_posterior_mean_condition}
        \frac{\partial}{\partial y_m} f_{n}(y) = \frac{\partial}{\partial y_{n}} f_m(y), \quad \forall n,m \text{ and } \forall y.
    \end{equation}    
    For notation simplicity, we let $e_{n, l}(x, y) = \exp \left( - \frac{\|y[\omega_{n}] - x[\omega_l]\|^2}{2\sigma^2}\right)$.
    We have the weights of the LE-MMSE estimator in~\cref{theorem:local_trans_estimator} becomes:
    \begin{equation*}
        w_{n, l} (x \vert y) = \exp \left( - \frac{\norm{y[\omega_{n}] - x[\omega_l]}}{2\sigma^2}\right) / Z_{n}(y) = \frac{e_{n, l}(x, y)}{Z_n(y)}.
    \end{equation*}
    where $Z_n(y) = \sum_{x' \in \D} \sum_{l'=1}^N e_{n, l'}(x', y)$ is the normalization constant.
    Therefore, the LE-MMSE estimator at pixel $n$ reads:
    \begin{equation*}
        f_{n}(y) = \frac{1}{Z_{n}(y)} \sum_{x\in \D} \sum_{l=1}^N x_l \cdot e_{n, l}(x, y).
    \end{equation*}
    That is:
    \begin{equation*}
        f_{n}(y) = \frac{S_{n}(y)}{Z_{n}(y)} \qquad \text{where} \qquad S_{n}(y) = \sum_{x\in \D} \sum_{l=1}^N x_l \cdot e_{n, l}(x, y).
    \end{equation*}
    A key observation is that \emph{this estimator is real-analytic for $\sigma>0$}. 
    To prove it, we remind that the product of polynomials and exponentials are real-analytic functions, and that the sum and quotient (with non-vanishing denominator) of real-analytic functions are also real-analytic.
    The denominator $Z_n(y)$ does not vanish for $\sigma>0$, which concludes the proof of real-analyticity.

	We will use the following classical result (see e.g. \citep[Section 3.1.24]{federer1996geometric} and \citep{mityagin2020zero} for an elementary proof):
    \begin{lemma}[Zeros of real-analytic functions]\label{lemma:zero_measure_analytic_eq}
		Let $f : \R^{p} \to \R$ be a real-analytic function for some $p \in \mathbb{N}^*$. If $f$ is not identically zero, then the set of zeros of $f$ has Lebesgue measure zero.
    \end{lemma}

    Applying this lemma, it therefore suffices to find a dataset $\D$ and a point $y$ such that $\frac{\partial}{\partial y_m} f_{n}(y) - \frac{\partial}{\partial y_{n}} f_m(y) \neq 0$.
    To this end, we consider the case of overlapping patches $\omega_n\cap \omega_m \neq \emptyset$ with $n \neq m$, which is always possible for patch sizes $P \geq 2$.
    Now consider the case where $n \in \omega_{m}$ and $m \in \omega_{n}$ (they are equivalent). 

    The chain rule gives 
    \[
        \frac{\partial e_{n, l}}{\partial y_m}(x, y) = -\sigma^{-2} (y_m - x_{l + m - n}) \cdot e_{n, l}(x, y) 
    \]
    and the quotient rule gives: 
    \begin{align*}
        &\frac{\partial w_{n, l}}{\partial y_m}(x, y) = \frac{\partial e_{n, l}}{\partial y_m}(x, y) \cdot \frac{1}{Z_{n}(y)} - e_{n, l}(x, y) \cdot \frac{\partial Z_{n}}{\partial y_m}(y) \cdot \frac{1}{Z_{n}(y)^2} \\
        &= -\sigma^{-2} (y_m - x_{l + m - n}) \cdot \frac{e_{n, l}(x, y)}{Z_{n}(y)} -  \frac{e_{n, l}(x, y)}{Z_{n}(y)^2} \cdot \frac{\partial Z_{n}(y)}{\partial y_m} \\
        &= -\sigma^{-2} (y_m - x_{l + m - n}) \cdot w_{n, l}(x, y) -  w_{n, l}(x, y) \cdot \frac{1}{Z_{n}(y)} \cdot \left( \sum_{x' \in \D} \sum_{l'=1}^{N} \frac{\partial e_{n, l'}}{\partial y_m}(x',y) \right) \\ 
        &= -\sigma^{-2} \left( (y_m - x_{l + m - n}) \cdot w_{n, l}(x, y) +  w_{n, l}(x, y) \cdot \frac{1}{Z_{n}(y)} \cdot \left( \sum_{x' \in \D} \sum_{l'=1}^{N} (y_m - x_{l' + m - n}) \cdot e_{n, l'}(x',y) \right) \right)\\
        &= -\sigma^{-2} \left((y_m - x_{l + m - n}) \cdot w_{n, l}(x, y) +  w_{n, l}(x, y) \cdot \frac{1}{Z_{n}(y)} \cdot \left( y_m \cdot Z_{n}(y) - \sum_{x' \in \D} \sum_{l'=1}^{N} x_{l' + m - n} \cdot e_{n, l'}(x',y) \right)\right) \\
        &=  \sigma^{-2} \cdot \left( x_{l + m - n} \cdot w_{n, l}(x, y) - w_{n, l}(x, y) \cdot \frac{1}{Z_{n}(y)} \cdot \sum_{x' \in \D} \sum_{l'=1}^{N} x_{l' + m - n} \cdot e_{n, l'}(x',y) \right)\\
        &=  \sigma^{-2} \cdot \left( x_{l + m - n} \cdot w_{n, l}(x, y) - w_{n, l}(x, y) \cdot \sum_{x' \in \D} \sum_{l'=1}^{N} x_{l' + m - n} \cdot w_{n, l'}(x',y) \right)\\
        &= \sigma^{-2} w_{n, l}(x, y)  \left( x_{l + m - n} - \bar{x}_n^{(m - n)}(y)  \right)
    \end{align*}
    where we define $\bar{x}_n^{(k)}(y) = \sum_{x' \in \D} \sum_{l'=1}^{N} x_{l' + k} \cdot w_{n, l'}(x',y)$ the local posterior mean of pixel $n$ with offset $k$. 
    Finally, differentiating $f_n(y)$ gives:
    \begin{align*}
        \frac{\partial f_{n}}{\partial y_m}(y) &= \sum_{x \in \D} \sum_{l=1}^N x_l \cdot \frac{\partial w_{n, l}}{\partial y_m}(x, y) \\
        &= \sigma^{-2} \sum_{x \in \D} \sum_{l=1}^N x_l \cdot w_{n, l}(x, y)  \left( x_{l + m - n} - \bar{x}_n^{(m - n)}(y)  \right) \\
        % &= \sigma^{-2} \sum_{x \in \D} \sum_{l=1}^N x_l \cdot x_{l + m - n} \cdot w_{n, l}(x, y) - \sigma^{-2} f_{n}(y) \cdot \bar{x}_n^{(m - n)}(y)
    \end{align*}

	Set $n = 2$ and $m=3$. 
    Set $\D = \{x, 0, \dots 0\}$ to be dataset containing only one non-zero image $x$ and the rest are zeros.
    We choose $x$ such that $x_2 > 0$ and $x_i = 0$ for $i \neq 2$.
	The derivatives simplify to:
	\begin{align*}
		\frac{\partial f_2}{\partial y_3}(y) &= \sigma^{-2}x_2 w_{2,2}(x, y) (x_3  - \bar{x}_2^{(1)}(y)) = -\sigma^{-2}x_2^2 w_{2,2}(x, y) w_{2,1}(x,y) \\
		\frac{\partial f_3}{\partial y_2}(y) &= \sigma^{-2} x_2 w_{3,2}(x, y) (x_1  - \bar{x}_3^{(-1)}(y)) = -\sigma^{-2}x_2^2 w_{3,2}(x,y)w_{3,3}(x,y)
	\end{align*}

	Fix an index $1 \leq i \leq N$ such that $y_i$ is a variable appearing in the vector $y[\omega_3]$ but not in the vector $y[\omega_2]$ (note that $i \neq 3$, otherwise $y_i = y_3$ is the center of the patch $y[\omega_3]$ and it's also in the patch $y[\omega_2]$ since the patch size $P > 1$). We have
	\begin{align*}
		\frac{\partial w_{2,2}(x,y)}{\partial y_i} = \frac{\partial w_{2,1}(x,y)}{\partial y_i} = 0 \quad \text{and} \quad \frac{\partial^2 f_2}{\partial y_3 y_i}(x,y) = 0.
	\end{align*}
	On the other hand, since $ i \neq 3$, we have
	\begin{align*}
		\frac{\partial w_{3,2}(x,y)}{\partial y_i} &= \sigma^{-2} w_{3, 2}(x, y)  \left( x_{2 + i - 3} - \bar{x}_3^{(i - 3)}(y)  \right) = -\sigma^{-2} x_2 w_{3, 2}(x, y) w_{3,2 - (i-3)}(x,y) \\
		\frac{\partial w_{3,3}(x,y)}{\partial y_i} &= \sigma^{-2} w_{3, 3}(x, y)  \left( x_{i } - \bar{x}_3^{(i - 3)}(y)  \right) = -\sigma^{-2} x_2 w_{3, 3}(x, y) w_{3,2 - (i-3)}(x,y). 
	\end{align*}
	We obtain
	\begin{align*}
		\frac{\partial^2 f_3(x,y)}{\partial y_2 y_i} &=\sigma^{-4} x_2^3 w_{3, 2}(x, y) w_{3,2 - (i-3)}w_{3,3}(x,y) +\sigma^{-4} x_2^3 w_{3, 3}(x, y) w_{3,2 - (i-3)}(x,y)w_{3,2} (x,y) > 0.
	\end{align*}
	Therefore, for this value of $x$, $\frac{\partial f_3}{\partial y_2}$ and $\frac{\partial f_2}{\partial y_3}$ are independent, when seen as functions of $y$. 
	We deduce that the equation $\frac{\partial f_2}{\partial y_3} = \frac{\partial f_3}{\partial y_2}$ 
	is a nondegenerate analytic equation in variables $\mathcal{D},y$. Using~\cref{lemma:zero_measure_analytic_eq} and by Fubini's theorem, the set $\left\{ \mathcal{D},\, \forall y,\, \frac{\partial f_2}{\partial y_3}(y) = \frac{\partial f_3}{\partial y_2}(y)\right\}$ also has zero measure.

    In other words, for almost every dataset $\D$, there exists $y$ such that the symmetry condition~\cref{eq:supp_posterior_mean_condition} does not hold, and the LE-MMSE estimator is not a posterior mean.
\end{proof}
% ###########################################################
% ###########################################################
% ###########################################################
% ###########################################################
% ###########################################################
% ###########################################################


% ###########################################################
%           NUMERICAL
% ########################################################### 
\null 
\pagebreak
\section{Numerical experiments}\label{sec:supp_numerical}
\subsection{Datasets}
We use the following datasets in our experiments: FFHQ~\citep{karras2019style} downscaled to $32 \times 32$ or $64 \times 64$, CIFAR-10~\citep{krizhevsky2009learning} and Fashion-MNIST~\citep{xiao2017fashion}.
For each dataset, we randomly select $10,000$ images for training, which is denoted by $\D$ in the main paper. 
\subsection{Network architectures}\label{sec:supp_network_architectures}
For the local and translation equivariant estimator, we examine 3 different architectures with noise level $\sigma$ conditioning:
\begin{itemize}
    \item UNet2D: we use a state-of-the-art UNet2D~\citep{ronneberger2015u} from the \texttt{diffusers}\footnote{\href{https://huggingface.co/docs/diffusers/en/api/models/unet2d}{https://huggingface.co/docs/diffusers/en/api/models/unet2d}} library. 
    The model has several downsampling and upsampling layers and skip connections, with varying channel dimensions (defined by \texttt{block\_out\_channels}) and kernel size (defined by \texttt{kernel\_size}, for down-blocks and mid-blocks). 
    The architecture is modified slightly (circular boundary conditions and kernel sizes of convolutional layers) to have a desired receptive field and to ensure translation equivariance. 
    The noise level $\sigma$ is conditioned using a time embedding module with linear layers.

    \item ResNet: we use a minimal ResNet with residual block~\citep{he2016deep}, with a $1 \times 1$ convolutional layer at the beginning and at the end, similar to~\citep{kamb2025an}. Each residual block contains a convolutional layer with $3 \times 3$ kernels at a channel dimension defined by \texttt{num\_channels}, followed by a ReLU nonlinearity. The noise level $\sigma$ is conditioned using sine-cosine positional embedding.    

    \item PatchMLP: a fully local MLP acting on patches. The MLP has 5 residual blocks, each containing two linear layers with hidden dimension of \texttt{hidden\_dim}, followed by a layer normalization and GELU activation. 
    The noise level $\sigma$ is conditioned using sine-cosine positional embedding. 
\end{itemize}
All convolutional layers use circular padding to ensure translation equivariance and they have approximately $4$ million parameters. 
Details of the architectures with various receptive fields are provided in~\cref{table:supp_architecture_details}.
    \begin{table}[t!]
        \caption{Details of neural network architectures with various receptive fields used in our experiments for images at $32 \times 32$ resolution.}
        \label{table:supp_architecture_details}
        \vspace*{-0.25cm}
        \begin{center}
        \begin{small}
        \begin{sc}
            \begin{tabular}{lccccc}
            \toprule
            & \multirow{2}{*}{\shortstack{Receptive field \\ (patch size)}} & \multicolumn{2}{c}{\multirow{2}{*}{Archi. hyper-parameters}} & \multirow{2}{*}{Num. parameters} \\
            & & & & \\
            \midrule
            % ---------------------------------------
            %         UNet2D
            % \multicolumn{4}{c}{UNet2D} \\
            UNet2D & & {\normalfont  \texttt{block\_out\_channels}} & {\normalfont  \texttt{kernel\_size}} & \\ 
            \midrule
            & $5$ & $(96, 224, 480)$     & $(3, 1, 1, 1)$--$1$ & 4.6M  \\
            & $7$ & $(64, 192, 448)$     & $(3, 1, 1, 1)$--$3$ & 5.1M \\
            & $9$ & $(96, 192, 288)$     & $(3, 1, 1, 1)$--$5$ & 4.3M \\
            & $11$ & $(64, 96, 160, 288)$     & $(3, 1, 1, 1, 3)$--$1$ & 4.3M \\
            % ---------------------------------------
            %         ResNet
            \midrule
            % \multicolumn{4}{c}{ResNet} \\
            ResNet & & {\normalfont  \texttt{num\_res\_blocks}} & {\normalfont  \texttt{num\_channels}} & \\ 
            \midrule
            & $5$ & $1$     & $640$ & 4.5M  \\
            & $7$ & $2$     & $448$ & 4.2M \\
            & $9$ & $3$     & $384$ & 4.5M \\
            & $11$ & $4$     & $328$ & 4.4M \\
            % ---------------------------------------
            %         MLP
            \midrule
            % \multicolumn{4}{c}{PatchMLP} \\
            PatchMLP & & {\normalfont  \texttt{hidden\_dim}} & {\normalfont  \texttt{num\_blocks}} & \\ 
            \midrule
            & $5$ & $7168$     & $5$ & 3.5M  \\
            & $7$ & $6144$     & $5$ & 3.7M \\
            & $9$ & $5120$     & $5$ & 4.3M \\
            & $11$ & $3072$     & $5$ & 4.0M \\
            \bottomrule
            \end{tabular}
        \end{sc}
        \end{small}
        \end{center}
        \vskip -0.1in
    \end{table} 

\subsection{Training procedure}\label{sec:supp_training_procedure}
All models are trained on a single NVIDIA A100 GPU with the following settings:
\begin{itemize}
    \item Optimizer: Adam optimizer~\citep{kingma2015adam}
    \item Learning rate: starting at $10^{-4}$ with cosine decay schedule and minimum learning rate at $10^{-6}$. 
    \item Number of epochs: $600$ for images at $32 \times 32$ resolution and $900$ for images at $64 \times 64$ resolution.
    \item Batch size: $256$
    \item Exponential moving average (EMA) with decay rate $0.99$ from the 1000-th training step and updates every $5$ steps. The EMA weights are used for evaluation.  
\end{itemize}
\subsection{Forward operators}
We consider the $3$ representative inverse problems as forward operators $A$:
\begin{itemize}
    \item Denoising: the forward operator is simply the identity $A = \Id$ and only the physics-agnostic estimator is applicable in this case.
    \item Inpainting: we consider the inpainting operator with a center square mask of size $15 \times 15$. The forward operator $A$ is therefore a diagonal matrix with $0$ on the masked pixels and $1$ elsewhere. 
    In this case, the pseudo-inverse is $BA^+  \!=\! A$: it simply removes noise inside the masked region and keeps the observed pixels unchanged.
    In our experiments, we consider the physics-aware estimator with $B = A^+ + \epsilon \Id$, where $\epsilon = 10^{-5}$ is a small regularization parameter. 
    This ensures that $B$ is full-rank and we can apply the constant-rank formula of the LE-MMSE estimator in~\cref{theorem:local_trans_estimator}. 
    As $\epsilon$ is very small, this estimator closely approximates the ideal physics-aware estimator with $B = A^+$, as discussed in~\cref{remark:rank_deficient_B}. 
    \item Deconvolution: we consider an isotropic Gaussian blur kernel with standard deviation $1.0$ with circular boundary conditions. The same kernel is used for all color channels. 
    In this case, we build the full matrix $A$ as a block-circulant matrix representing the convolution operation and $B$ is its pseudo-inverse, which is computed once, in double precision.  
\end{itemize}
\vfill
\subsection{Analytical formula implementation}
Implementing the analytical formulas of the E-MMSE~\cref{thm:equivariant_mmse} and the LE-MMSE~\cref{theorem:local_trans_estimator} estimators requires computing many distance terms between images or patches in the dataset $\D$ and the observed measurement $y$. We process by batch to avoid memory overflow.
For numerical stability and avoidance of overflow/underflow, the exponential terms are accumulated using an online log-sum-exp trick.
All theoretical estimators are computed in an \emph{exact} manner without any approximation, modular finite precision arithmetic. 
We use PyTorch~\citep{paszke2019pytorch} for implementation and all computations are performed in single precision (FP32) on a single A-100 GPU, unless otherwise specified.
All estimators are implemented, even the rank-deficient cases. 
Full implementation details are available at \url{\gitrepo}.

\pagebreak
\section{Additional numerical results}
\subsection{Comparison between trained neural networks and the analytical LE-MMSE estimator}\label{sec:supp_neural_vs_analytical}
We show in~\cref{fig:neural_vs_analytical_unet2d_patch_11,fig:neural_vs_analytical_resnet_patch_11,fig:neural_vs_analytical_patchmlp_patch_11} additional PSNR results between trained neural networks and the analytical LE-MMSE estimator for different architectures (UNet2D, ResNet, PatchMLP) on both training and test sets of various datasets. 
We recover a consistent conclusion across different settings: the trained neural networks closely approximate the analytical LE-MMSE estimator, with PSNR values exceeding $20$ in most cases and often reaching above $30$ dB.
% PSNR results for different architectures with patch size 11
\begin{figure*}[ht!]
    \centering
    \def\base{./images/neural_vs_analytical/localequivunet2dcondmodel_patch_11/plots}
    \begin{minipage}{0.95\linewidth}
        \includegraphics[width=\linewidth]{\base/FFHQ_images32x32_subset_10000_combined_psnr_grid.pdf}
        \subcaption{FFHQ-32}
    \end{minipage}
    \begin{minipage}{0.95\linewidth}
        \includegraphics[width=\linewidth]{\base/CIFAR10_subset_10000_combined_psnr_grid.pdf}
        \subcaption{CIFAR-10}
    \end{minipage}
    \begin{minipage}{0.95\linewidth}
        \includegraphics[width=\linewidth]{\base/FashionMNIST_subset_10000_combined_psnr_grid.pdf}
        \subcaption{Fashion-MNIST}
    \end{minipage}
    \caption{Additional PSNR between trained UNet2D and the analytical formula of the LE-MMSE for different inverse problems on both training and test sets of various datasets. 
    The patch size is $P = 11 \times 11$.
    Left: physics-agnostic estimator with $B\!=\!\Id$. Right: physics-aware estimator with $B\!=\!A^+$.
    We recover the same conclusions as in~\cref{fig:neural_vs_analytical_unet2d_patch_5}.
    }
    \label{fig:neural_vs_analytical_unet2d_patch_11} 
\end{figure*}

\begin{figure*}[h!]
    \centering
    \def\base{./images/neural_vs_analytical/minimalresnet_patch_11/plots}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/FFHQ_images32x32_subset_10000_combined_psnr_grid.pdf}
        \subcaption{FFHQ-32}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/CIFAR10_subset_10000_combined_psnr_grid.pdf}
        \subcaption{CIFAR-10}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/FashionMNIST_subset_10000_combined_psnr_grid.pdf}
        \subcaption{Fashion-MNIST}
    \end{minipage}
    \caption{PSNR between trained ResNet and the analytical formula of the LE-MMSE for different inverse problems on both training and test sets of various datasets. 
    The patch size is $P = 11 \times 11$.
    Left: physics-agnostic estimator with $B\!=\!\Id$. Right: physics-aware estimator with $B\!=\!A^+$.
    We recover the same conclusions accross architectures and settings.
    }
    \label{fig:neural_vs_analytical_resnet_patch_11} 
\end{figure*}

\begin{figure*}[h!]
    \centering
    \def\base{./images/neural_vs_analytical/patchmlp_patch_11/plots}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/FFHQ_images32x32_subset_10000_combined_psnr_grid.pdf}
        \subcaption{FFHQ-32}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/CIFAR10_subset_10000_combined_psnr_grid.pdf}
        \subcaption{CIFAR-10}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{\base/FashionMNIST_subset_10000_combined_psnr_grid.pdf}
        \subcaption{Fashion-MNIST}
    \end{minipage}
    \caption{PSNR between trained PatchMLP neural network and the analytical formula of the LE-MMSE for different inverse problems on both training and test sets of various datasets.      
    The patch size is $P = 11 \times 11$.
    We recover the same conclusions as in~\cref{fig:neural_vs_analytical_unet2d_patch_5,fig:neural_vs_analytical_unet2d_patch_11,fig:neural_vs_analytical_resnet_patch_11}. An exception is observed for the deconvolution task with physics-aware models (rightmost column). Here, the noise is amplified by the inversion of the blurring operator and the local PatchMLP architecture struggles to accurately reconstruct fine details, leading to a lower PSNR comparing to CNNs. We hypothesize that CNNs have other inductive biases that are more suited to handle such challenging tasks.    
    }
    \label{fig:neural_vs_analytical_patchmlp_patch_11} 
\end{figure*}
\null
\pagebreak

% # Qualitative comparison for different patch sizes
\subsection{Addition qualitative comparison}
We provide additional qualitative comparisons between the analytical LE-MMSE estimator and trained neural networks (UNet2D, ResNet, PatchMLP) for different patch sizes in~\cref{fig:qualitative_neural_vs_analytical_patch_5,fig:qualitative_neural_vs_analytical_patch_7,fig:qualitative_neural_vs_analytical_patch_9,fig:qualitative_neural_vs_analytical_patch_11} on the FFHQ, CIFAR10 and FashionMNIST datasets.
Overall, the neural networks closely match the analytical solution on both training and test sets across all datasets and inverse problems, confirming our theoretical findings. 
\begin{figure*}[ht!]
        \centering
        \vskip -0.1in
        \includegraphics[trim=35mm 0 0 0, clip,width=\textwidth]{./tex_figure/fig_qualitative_neural_vs_analytical_patch_5_standalone.pdf}
        \vskip -0.1in
        \caption{Qualitative comparison. 
        The patch size is $P = 5 \times 5$ and $B\!=\!\Id$.
        The noise level is $\sigma = 0.05,0.2,0.8$ from left to right for each column.
        The neural networks (UNet2D, ResNet, and PatchMLP) closely match the analytical LE-MMSE on both training and test sets across all datasets (FFHQ, CIFAR10, and FashionMNIST) and inverse problems, confirming our theoretical findings. 
        }
        \label{fig:qualitative_neural_vs_analytical_patch_5} 
        \vskip -0.15in
\end{figure*}

\begin{figure*}[ht!]
        \centering
        \vskip -0.1in
        \includegraphics[trim=35mm 0 0 0, clip,width=\textwidth]{./tex_figure/fig_qualitative_neural_vs_analytical_patch_7_standalone.pdf}
        \vskip -0.15in
        \caption{Qualitative comparison between the analytical formula LE-MMSE, UNet2D, ResNet, and PatchMLP, with $B\!=\!\Id$ on FFHQ, CIFAR10 and FashionMNIST. 
        The patch size is $P=7 \times 7$.
        The noise level is $\sigma = 0.05,0.2,0.8$ from left to right for each column.
        The neural networks closely match the analytical solution on both training and test sets across all datasets and inverse problems, confirming our theoretical findings. 
        Yet, some discrepancies can be observed, especially at low noise levels on the test set, which we attribute to generalization issues discussed in~\cref{sec:neural_generalization}.
        }
        \label{fig:qualitative_neural_vs_analytical_patch_7} 
        \vskip -0.15in
\end{figure*}

\begin{figure*}[ht!]
        \centering
        \vskip -0.1in
        \includegraphics[trim=35mm 0 0 0, clip,width=\textwidth]{./tex_figure/fig_qualitative_neural_vs_analytical_patch_9_standalone.pdf}
        \vskip -0.15in
    \caption{Qualitative comparison between the analytical formula LE-MMSE, UNet2D, ResNet, and PatchMLP, with $B\!=\!\Id$ on FFHQ, CIFAR10 and FashionMNIST. 
    The patch size is $P=9$.
    The noise level is $\sigma = 0.05,0.2,0.8$ from left to right for each column.
    The neural networks closely match the analytical solution on both training and test sets across all datasets and inverse problems, confirming our theoretical findings. 
    Yet, some discrepancies can be observed, especially at low noise levels on the test set, which we attribute to generalization issues discussed in~\cref{sec:neural_generalization}.
    }
	\label{fig:qualitative_neural_vs_analytical_patch_9} 
    \vskip -0.15in
\end{figure*}

\begin{figure*}[ht!]
        \centering
        \vskip -0.1in
        \includegraphics[trim=35mm 0 0 0, clip,width=\textwidth]{./tex_figure/fig_qualitative_neural_vs_analytical_patch_11_standalone.pdf}
        \vskip -0.15in
    \caption{Qualitative comparison between the analytical formula LE-MMSE, UNet2D, ResNet, and PatchMLP, with $B\!=\!\Id$ on FFHQ, CIFAR10 and FashionMNIST. 
    The patch size is $P=11$.
    The noise level is $\sigma = 0.05,0.2,0.8$ from left to right for each column.
    The neural networks closely match the analytical solution on both training and test sets across all datasets and inverse problems, confirming our theoretical findings. 
    Yet, some discrepancies can be observed, especially at low noise levels on the test set, which we attribute to generalization issues discussed in~\cref{sec:neural_generalization}.
    }
	\label{fig:qualitative_neural_vs_analytical_patch_11} 
    \vskip -0.15in
\end{figure*}

\null
\pagebreak
\null
\pagebreak
\null
\pagebreak
\subsection{Influence of the patch size (receptive field)}\label{sec:supp_patch_size}
\paragraph{Density of the patch distribution}
We analyze here the influence of the patch size on the density of the patch distribution in FFHQ-32 dataset. 
We compute the negative-log-density of patches as a function of the patch size, for points on either the training set or the test set.  
The patch density is exactly the denominator term in the analytical LE-MMSE formula~\cref{eq:loc_equiv_formula}.
The results are shown in~\cref{fig:supp_patch_size_vs_density}.
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=\linewidth]{./images/analytical_density_vs_patch/denoising_FFHQ_images32x32_subset_10000_patches_5-7-9-11-15-19_train.pdf}
    \includegraphics[width=\linewidth]{./images/analytical_density_vs_patch/denoising_FFHQ_images32x32_subset_10000_patches_5-7-9-11-15-19_test.pdf}
    \vskip -0.1in
    \caption{The negative-log-density of patches in FFHQ-32 training set (top) and test set (bottom) as a function of the patch size.
    As the patch size increases, the density of patches decreases significantly, indicating a sparser coverage of the patch space by the dataset. 
    In the training set, the negative-log-density is relatively lower (around $10^2$) than in the test set (around $10^4 $ for small noise levels), indicating a better coverage of the patch space by the training set. 
    This observation helps explain the drop in PSNR between trained neural networks and the analytical LE-MMSE estimator in the test set for low noise levels.
    }
    \label{fig:supp_patch_size_vs_density}
\end{figure}

\null 
\pagebreak
\paragraph{Patch size influence}
In~\cref{fig:supp_neural_analytic_vs_patch_size}, we show the PSNR between the trained UNet2D and the analytical LE-MMSE estimator for different patch sizes on both training and test sets of FFHQ-32 dataset. 
For low noise levels and in the test set, the PSNR decreases as the patch size increases, which we attribute to the sparser coverage of the patch space by the dataset for larger patches, as shown in~\cref{fig:supp_patch_size_vs_density}. 

\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=.75\linewidth]{./images/neural_vs_analytical/localequivunet2dcondmodel_patch_5_7_9_11/plots/FFHQ_images32x32_subset_10000_patch_grid_nva_only.pdf}
    \vskip -0.1in
    \caption{PSNR between trained UNet2D and the analytical formula of the LE-MMSE versus the patch size, on the FFHQ-32 dataset.
    Top: on the training set. Bottom: on the test set.
    }
    \label{fig:supp_neural_analytic_vs_patch_size}
\end{figure}

In~\cref{fig:supp_analytic_vs_gt_patch_size}, we show the PSNR between the analytical LE-MMSE estimator and the ground truth as a function of the patch size on both training and test sets of FFHQ-32 dataset.
We observe that -- on the test set -- smaller patch size are preferable for low noise levels, while larger patch sizes yield better performance for high noise levels.
The behavior on the training set is different: for low noise levels, the analytical formula copy-pastes exactly the right patches, explaining the blow up of PSNR at the origin.

\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=.75\linewidth]{./images/rec_perf/localequivunet2dcondmodel_patch_5_7_9_11/plots/FFHQ_images32x32_subset_10000_patch_grid_analytical_vs_gt.pdf}
    \vskip -0.1in
	\caption{PSNR between the analytical formula of the LE-MMSE and the ground truth versus the patch size, on the FFHQ-32 dataset. 
    Top: on the training set. Bottom: on the test set.
    }
    \label{fig:supp_analytic_vs_gt_patch_size}
\end{figure}

% \null
% \pagebreak

\subsection{Mass concentration}\label{sec:supp_mass_concentration}
The LE-MMSE estimator~\cref{theorem:local_trans_estimator} is a weighted average of the central pixel values of patches in the dataset. 
To understand the behavior of the estimator, we analyze how many patches contribute significantly to the estimate at each pixel location.
In~\cref{fig:supp_mass_concentration}, we show the number of patches contributing to $99\%$ of the mass of the LE-MMSE estimator as a function of the noise level $\sigma$.
We observe that for low noise levels, the estimator concentrates its mass on fewer patches (even nearest neighbors).
As the noise level increases, the number of contributing patches increases significantly, with a critical value of $\sigma$ where the number of patches starts to increase rapidly.
This behavior shows that although the MMSE estimator is typically associated with averaging, it can behave like a nearest-neighbor estimator when the noise is small, due to the strong concentration of mass on a very limited subset of patches.
We also observe a clear difference between physics-agnostic and physics-informed settings for deconvolution, where the latter has a significantly higher number of contributing patches due to the amplification of noise by the pseudo-inverse of the blurring operator.
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=0.6\columnwidth]{./images/patch_works/patch_works_effective_samples_FFHQ_images32x32_p5.pdf}
    \vskip -0.1in
    \caption{
    For low noise levels, the LE-MMSE estimator concentrates its mass on fewer patches (even nearest neighbors).
    Here we show the median and IQR (over all pixels of $50$ samples on the test set, for each $\sigma$) of the number of patches contributing to $99\%$ of the mass of the LE-MMSE estimator. 
    There is a significant increase in number of contributed patches as the noise level $\sigma$ increases, 
    and a critical value of $\sigma$ where the number of patches starts to increase rapidly.
    The patch size is $P = 5 \times 5$, on FFHQ-32.
    Note that due to computational constraints, we only keep the top $10^4$ nearest patches (over $32 \times 32 \times 10^4 \approx 10^7$ patches) when computing mass concentration, but the estimator is still exact as we accumulate all the mass in an online manner.
    }
    \label{fig:supp_mass_concentration}
    \vskip -0.1in
\end{figure}
We also visualize in~\cref{fig:supp_mass_concentration_visual,fig:supp_mass_concentration_visual_inform} the $\log_{10}$ of number of patches contributing to $99\%$ of the mass of the LE-MMSE estimator at each pixel location for different inverse problems on FFHQ-32 dataset.
We observe that for low noise levels, the nearest patch (or a very small number of patches) contributes $99\%$ of the mass at each pixel location. 
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \def\root{./images/patch_works_visuals/visual_FFHQ_images32x32}
    \def\size{0.95\linewidth}
    \begin{minipage}{\size}
        \includegraphics[width=\linewidth]{\root_denoising_agnostic_img0_p5.pdf}
        \subcaption{Denoising}
    \end{minipage}
    \begin{minipage}{\size}
        \includegraphics[width=\linewidth]{\root_inpainting_center_15_agnostic_img1_p5.pdf}
        \subcaption{Inpainting (physic-agnostic)}
    \end{minipage}
    \begin{minipage}{\size}
        \includegraphics[width=\linewidth]{\root_convolution_gaussian_1.0_agnostic_img2_p5.pdf}
        \subcaption{Deconvolution (physic-agnostic)}
    \end{minipage}
    % \begin{minipage}{\size}
    %     \includegraphics[width=\linewidth]{\root_convolution_gaussian_1.0_inform_img2_p5.pdf}
    %     \subcaption{Deconvolution (physic-informed)}
    % \end{minipage}
    % \begin{minipage}{\size}
    %     \includegraphics[width=\linewidth]{\root_inpainting_center_15_inform_img1_p5.pdf}
    %     \subcaption{Inpainting (physic-informed)}
    % \end{minipage}
    \caption{Visualization of the number of patches (in $\log_{10}$ scale) contributing to $99\%$ of the mass of the physic-agnostic LE-MMSE estimator at each pixel location for different inverse problems on FFHQ-32 dataset.
    }\label{fig:supp_mass_concentration_visual}
    \vskip -0.1in
\end{figure}
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \def\root{./images/patch_works_visuals/visual_FFHQ_images32x32}
    \def\size{0.95\linewidth}
    \begin{minipage}{\size}
        \includegraphics[width=\linewidth]{\root_inpainting_center_15_inform_img1_p5.pdf}
        \subcaption{Inpainting (physic-informed)}
    \end{minipage}
    \begin{minipage}{\size}
        \includegraphics[width=\linewidth]{\root_convolution_gaussian_1.0_inform_img2_p5.pdf}
        \subcaption{Deconvolution (physic-informed)}
    \end{minipage}
    \caption{Visualization of the number of patches (in $\log_{10}$ scale) contributing to $99\%$ of the mass of the physic-inform LE-MMSE estimator at each pixel location for different inverse problems on FFHQ-32 dataset.
    For deconvolution, the number of contributing patches is significantly higher than for the physic-inform case, due to the amplification of noise by the pseudo-inverse of the blurring operator.
    }
    \vskip -0.1in
    \label{fig:supp_mass_concentration_visual_inform}
\end{figure}

\null
\pagebreak
\null
\pagebreak
\subsection{LE-MMSE is a patchwork}\label{sec:supp_patch_works}
From the LE-MMSE formula~\cref{theorem:local_trans_estimator}, we see that the estimate at each pixel location is a weighted average of the central pixel values of patches in the dataset.
In fact, the estimator often concentrates its mass on very few patches, as shown in~\cref{sec:supp_mass_concentration}. 
Moreover, contiguous pixels may come from the central pixels of patches of the same image in the dataset, leading to a patchwork behavior. 

We visualize in~\cref{fig:supp_patch_index_ffhq,fig:supp_patch_index_FashionMNIST} this behavior for different inverse problems on FFHQ-32 and FashionMNIST datasets. 
More precisely, at each pixel location, the source index is the index of the image in the dataset that contains the patch whose central pixel contributes at least $50\%$ of the mass of the LE-MMSE estimator at that pixel location.
We observe that, large contiguous regions of the estimate come from the same source image in the dataset, which we can interpret as the estimator to be a patchwork of training patches.
As the noise level increases, more pixels become white, meaning that the corresponding pixel value is not mostly due to a single patch: the ``patchwork effect'' diminishes, as more patches contribute to the estimate at each pixel location. 
\begin{figure*}[ht!]
    \centering
    % \vskip -0.1in
    \includegraphics[trim=10mm 0 0 0,clip,width=0.9\linewidth]{./tex_figure/fig_patch_index_patch_11_standalone.pdf}
    \vskip -0.1in
    \caption{Illustration of the patchwork behavior of the LE-MMSE estimator (with $B\!=\!\Id$) on FFHQ-32 dataset for different inverse problems and noise levels. We use a patch size of $P = 11 \times 11$. 
    We observe contiguous regions coming from the same source image in the dataset for low noise levels.
    White pixels correspond to locations where no patch contributes more than $50\%$ of the mass.
    }
    \label{fig:supp_patch_index_ffhq}
\end{figure*}
\begin{figure*}[ht!]
    \centering
    \includegraphics[trim=10mm 0 0 0,clip,width=0.9\linewidth]{./tex_figure/fig_patch_index_patch_11_fashion_mnist_standalone.pdf}
    \vskip -0.1in
    \caption{Similar illustration as in~\cref{fig:supp_patch_index_ffhq} but on FashionMNIST dataset.}
    \label{fig:supp_patch_index_FashionMNIST}
\end{figure*}

Note that we use a patch size of $P = 11 \times 11$ to better visualize the patchwork behavior. 
This choice results in slightly weaker reconstruction performance, as discussed in~\cref{sec:supp_patch_size}.

\null
\pagebreak
\subsection{Out-of-distribution data}\label{sec:supp_ood}
We provide additional results on out-of-distribution (OOD) data using UNet2D architecture with receptive field of size $5 \times 5$.
The models and the formula are trained / evaluated on $\D=$ FFHQ-32. They are then evaluated on OOD dataset $\D'=$ CIFAR10. 
We show in~\cref{fig:supp_ood_ffhq_cifar10} the PSNR between trained UNet2D and the analytical LE-MMSE estimator.
For large noise levels, as the Gaussians overlap significantly even on OOD data, the value of $-\log p(y)$ is low and the trained neural network closely matches the analytical LE-MMSE estimator with PSNR values above $25$ dB. 
For low noise levels, the PSNR is about $3$ dB lower than in the in-distribution case shown in~\cref{fig:neural_vs_analytical_unet2d_patch_5}, which we attribute to the low-density of the measurement density $p(y)$, as discussed in~\cref{sec:neural_generalization} and can be seen in~\cref{fig:supp_ood_ffhq_cifar10}.
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=0.65\columnwidth]{./images/ood/localequivunet2dcondmodel_patch_5/ood_psnr_vs_sigma_FFHQ_images32x32.pdf} \\
    \includegraphics[width=0.65\columnwidth]{./images/ood/localequivunet2dcondmodel_patch_5/ood_density_vs_sigma_FFHQ_images32x32.pdf}
    \vskip -0.1in
    \caption{Comparison of UNet2D and the analytical LE-MMSE estimator on OOD dataset CIFAR10 when both are trained on FFHQ-32.
    Median and IQR using $50$ images per $\sigma$, $P = 5\times 5$ and $B\!=\!\Id$.
    On top: PSNR between UNet2D and the analytical LE-MMSE estimator.
    On bottom: negative-log-density of measurements $y$ in CIFAR10 under the measurement distribution induced by FFHQ-32. 
    }
    \label{fig:supp_ood_ffhq_cifar10}
\end{figure}

\null
\pagebreak
\subsection{Dataset size influence}\label{sec:supp_experiment_dataset_size}
We analyze here the influence of the dataset size on the alignment between trained neural networks and the analytical LE-MMSE estimator.
We train UNet2D models with receptive field of size $P = 5 \times 5$ and $B\!=\!\Id$ on various dataset sizes from $10^3$ to $5 \times 10^4$ images from FFHQ-32.
The PSNR between trained UNet2D and the analytical LE-MMSE estimator is reported in~\cref{fig:dataset_size_influence}.
We observe that the dataset size has limited influence on the alignment between trained neural networks and the analytical LE-MMSE formula, with a slight improvement when increasing the dataset size. 
\begin{figure}[ht!]
    \centering
    \vskip -0.1in
    \includegraphics[width=0.75\columnwidth]{./images/neural_vs_analytical/localequivunet2dcondmodel_patch_5/plots/FFHQ_images32x32full70k_sizes_1000_5000_10000_20000_30000_40000_50000_patch_5_psnr_vs_sigma_train_test.pdf}
    \vskip -0.1in
    \caption{
        Dataset size has limited influence on the alignment between neural networks and the LE-MMSE formula.
        Median and IQR using $50$ images per $\sigma$, $P = 5\times 5$ and $B\!=\!\Id$.
    }
    \label{fig:dataset_size_influence}
    \vskip -0.1in
\end{figure}

\null
\pagebreak
\subsection{Results on $3 \times 64 \times 64$ images}\label{sec:supp_experiment_64x64}
We provide additional results on images at $3 \times 64 \times 64$ resolution using UNet2D architecture with receptive field of size $11 \times 11$.
The models are trained on $10^4$ images from FFHQ downscaled to $64 \times 64$, with the same training procedure as in~\cref{sec:supp_training_procedure}.
    \begin{table}[ht!]
        \caption{Details of neural network architectures with various receptive fields used in our experiments for images at $3 \times 64 \times 64$ resolution.}
        \label{table:supp_architecture_unet2d_64}
        \vspace*{-0.25cm}
        \begin{center}
        \begin{small}
        \begin{sc}
            \begin{tabular}{lccccc}
            \toprule
            & \multirow{2}{*}{\shortstack{Receptive field \\ (patch size)}} & \multicolumn{2}{c}{\multirow{2}{*}{Archi. hyper-parameters}} & \multirow{2}{*}{Num. parameters} \\
            & & & & \\
            \midrule
            % ---------------------------------------
            %         UNet2D
            % \multicolumn{4}{c}{UNet2D} \\
            UNet2D & & {\normalfont  \texttt{block\_out\_channels}} & {\normalfont  \texttt{kernel\_size}} & \\ 
            \midrule
            & $11$ & $(64, 128, 256, 512)$     & $(3, 1, 1, 1, 3)$--$1$ & 13.3M \\
            \bottomrule
            \end{tabular}
        \end{sc}
        \end{small}
        \end{center}
        \vskip -0.1in
    \end{table}

\begin{figure*}[ht!]
    \centering
    % \vskip -0.1in
    \includegraphics[trim=16mm 0 0 0,clip,width=\linewidth]{./tex_figure/fig_qualitative_neural_vs_analytical_patch_11_64x64_more_standalone.pdf}
    \caption{Additional qualitative comparison between UNet2D and the analytical LE-MMSE estimator on FFHQ-64 across tasks.}
    \label{fig:supp_additional_qualitative_64x64}
\end{figure*}
% ########################################################################################################
